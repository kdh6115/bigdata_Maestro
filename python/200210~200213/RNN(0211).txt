RNN(Recurrent Neural Network)
- 순서보장(순서가 있는 데이터 특징 추출)
- cell이 여러개로 모여있는 구조 ( 첫 cell계산하고 그다음 cell 계산할때 시간지연있음)
	- cell에 들어가는 input(vector), 그리고 가중치와 계산되어서 나가지는 것이 특성
	- 그 특성이 다음  cell로 전달되어지는데 이걸 state
	- state와 그다음 cell에 들어오는 input data랑 계산하고 특성 뽑고 순차적으로 계속 이루어짐
	- 처리가 길어지면 처음 cell의 영향력이 떨어저 기울기 소실문제 발생, 뒷쪽에는 영향력을 너무 받음
		=> 이 문제를 해결하기 위해 LSTM
			- LSTM 두개의 회로 state_c, state_h
			- seq2seq  : translation, chatbot, summarization
			- 너무 비싸서 GRU 사용 (거의 LSTM과 유사, 계산이 작다)

<RNN으로 하려는 것>
1. Time Series 다루는것

[TimeData Generator]
- 시간을 sequence데이터로 하나만 나타낸다 (1960 1961 .... 2020)
			         	       (100    102 ...   200)
- 시계열 분석은 자기상관성을 가진다
- 10년째 데이터 보고(window) 그다음해에 있는 것을 target 그 다음해의 다음해는 10년째 데이터 한칸 이동해서 target으로 잡음
ex) 1960~1969 : window 1970 :target // 1961~1970 : window, 1971 : target
- 정상성이 띈 데이터로만 시계열 적용 가능
	- 평균, 분산, 공분산이 일정해야 한다. 
	- MA(이동평균법), AR(자동회귀), ARMA(AR+MA), ARIMA(best)
	- 비정상성 data -> 정상성 data : 차분(미분한다. (뒤에 데이터에서 앞의 데이터 뺌))

2. Text mining 다루는 것
ex) I have a pen -> tokenization(하나하나 자른다) -> dictionary(중복되지 않게 해줌) -> I / have / a / pen -> (0, 1, 2, 3) [one-hot encoding되어진다] -> model -> 분류
									0    1     2     3	    vectorization 근처에 있는 것 끼리 벡터를 준다.(word2vec)

	=> I have a book => dictionary => (0, 1, 2, )

- skip gram(중심단어를 이용해 주변 단어 예측) 두 단어가 같이 오는 것
- keras에서도 embeding이라는 layer로 vectorization 지원
- 숫자화해서 index를 해서 몇개나왔는지 cnt해줌
- 유사도를 이용


<중요도 학습 - Attention> - 가중치 : <query, key(attention point 구함)>, value(attention point로 attention value 구함) 
- NMT(번역) - LSTM + Attention망

<transformer> - LSTM을 없애고 해도 좋은 결과가 나와서 Attention망으로만 더 깊게 해서 특징추출해서 만듬
- BERT

https://reniew.github.io/22/     (skip gram)