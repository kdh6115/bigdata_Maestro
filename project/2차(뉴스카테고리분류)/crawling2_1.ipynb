{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ICT01_21\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ICT01_21\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ICT01_21\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ICT01_21\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ICT01_21\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ICT01_21\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\ICT01_21\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ICT01_21\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ICT01_21\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ICT01_21\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ICT01_21\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ICT01_21\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Twitter\n",
    "from gensim.models import Word2Vec\n",
    "import csv\n",
    "from konlpy.tag import Twitter\n",
    "from konlpy.tag import Hannanum\n",
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Komoran\n",
    "from konlpy.tag import Okt\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import csv\n",
    "import logging\n",
    "from konlpy import jvm\n",
    "logger = logging.getLogger(__name__)\n",
    "jvm.init_jvm()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = pd.read_excel(\"data.xlsx\")\n",
    "data_set = data_set.iloc[:,1:]\n",
    "data_set.rename({0:'news'},axis=1, inplace= True)\n",
    "data_label = {1:'culture',2:'global',3:'politic',4:'society',5:'economy'}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ICT01_21\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data_set)) :\n",
    "    data_set['news'][i] = re.sub('[^a-zA-Z0-9 ㄱ-ㅣ가-힣]', '', data_set['news'][i])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = data_set.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = data_set.iloc[:40000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1810만명이 민주당 경선 보자 트럼프 DMZ 흥행쇼 던졌다</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>단독 건보공단 N분의1 성과급 안고치면 내년엔 안준다</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>올 최강 태풍 하기비스 일본 강타할 듯동해안 간접영향</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>스브스뉴스 고래사냥 다시 시작한 그들을 막아선 해적</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>한국당 뺀 여야 518 논란 김진태이종명김순례 징계 추진</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                news  label\n",
       "0  1810만명이 민주당 경선 보자 트럼프 DMZ 흥행쇼 던졌다      2\n",
       "1      단독 건보공단 N분의1 성과급 안고치면 내년엔 안준다      5\n",
       "2      올 최강 태풍 하기비스 일본 강타할 듯동해안 간접영향      1\n",
       "3       스브스뉴스 고래사냥 다시 시작한 그들을 막아선 해적      2\n",
       "4    한국당 뺀 여야 518 논란 김진태이종명김순례 징계 추진      3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "news     0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noun(text):\n",
    "    tokenizer = Twitter()\n",
    "    nouns = tokenizer.nouns(text)\n",
    "    return [n for n in nouns]\n",
    "\n",
    "def get_noun2(text):\n",
    "    tokenizer = Hannanum()\n",
    "    nouns = tokenizer.nouns(text)\n",
    "    return [n for n in nouns]\n",
    "\n",
    "def get_noun3(text):\n",
    "    tokenizer = Okt()\n",
    "    nouns = tokenizer.nouns(text)\n",
    "    return [n for n in nouns]\n",
    "\n",
    "def get_morphs3(text):\n",
    "    tokenizer = Okt()\n",
    "    morphs = tokenizer.morphs(text)\n",
    "    return [n for n in morphs]\n",
    "\n",
    "def get_noun4(text):\n",
    "    tokenizer = Komoran()\n",
    "    nouns = tokenizer.nouns(text)\n",
    "    return [n for n in nouns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(data_set, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=train_set.reset_index(drop=True)\n",
    "test_set=test_set.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit_transform = CV.fit_transform(train_set[\"news\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#label_set = data_set['label']\n",
    "label_set = train_set['label']\n",
    "len(label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>유사군복 판매자 처벌 합헌 결정 후폭풍</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>북한 목선 책임 8군단장 해임합참의장 등 엄중 경고</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>교직 관두고 일 도우라 부친 부름에글로벌 기업으로 이끈 구자</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>월드리포트 시진핑은 정말 평양 답방을 주저하는 걸까</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>베트남 아내 짓밟은 한국인 남편참다 못해 몰래 찍었다</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27995</th>\n",
       "      <td>비하인드 뉴스 이낙연 총리가 직접 밝힌 큰절의 이유</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27996</th>\n",
       "      <td>7인조 아이돌 머스트비 교통사고운전하던 매니저 사망</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27997</th>\n",
       "      <td>화성연쇄살인 용의자 몽타주와 다르다처제 살인 때 못 알아봐</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27998</th>\n",
       "      <td>정경심 교수 구속법원 범죄 혐의 상당 부분 소명</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27999</th>\n",
       "      <td>소병철 영입하며 개혁 박차한국당 인사 앞둔  경고</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    news  label\n",
       "0                  유사군복 판매자 처벌 합헌 결정 후폭풍      3\n",
       "1           북한 목선 책임 8군단장 해임합참의장 등 엄중 경고      3\n",
       "2      교직 관두고 일 도우라 부친 부름에글로벌 기업으로 이끈 구자      5\n",
       "3           월드리포트 시진핑은 정말 평양 답방을 주저하는 걸까      2\n",
       "4          베트남 아내 짓밟은 한국인 남편참다 못해 몰래 찍었다      4\n",
       "...                                  ...    ...\n",
       "27995       비하인드 뉴스 이낙연 총리가 직접 밝힌 큰절의 이유      3\n",
       "27996       7인조 아이돌 머스트비 교통사고운전하던 매니저 사망      4\n",
       "27997   화성연쇄살인 용의자 몽타주와 다르다처제 살인 때 못 알아봐      4\n",
       "27998         정경심 교수 구속법원 범죄 혐의 상당 부분 소명      4\n",
       "27999        소병철 영입하며 개혁 박차한국당 인사 앞둔  경고      3\n",
       "\n",
       "[28000 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SUV차량 하천 다리 밑 추락필리핀행 여객기 회항</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BMW서 헬기 수준 굉음 고통 호소업체 결함 아냐</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>작년에 산 롱패딩이 무색하게올 겨울 왜 이상고온일까</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>독도서 첫 블랙 워터 다이빙한밤 수중 생물 조사</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>흔들리는 무기 빅딜 훈련기 수출 백지화 우려도 박수찬의</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>군산에 묻어달라던 서양인이 125년 전 남긴 사진</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>전광훈 황교안 정치가 아니다아무리 가르쳐도 안돼</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>한 명이라도 더 살려야끝까지 현장 지킨 응급의료 버팀목</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>단독 의사 접대 자리에서 성폭력임직원은 쉬쉬</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>카톡 보고에 즉답젊은 총수시대 왔다</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  news  label\n",
       "0          SUV차량 하천 다리 밑 추락필리핀행 여객기 회항      3\n",
       "1          BMW서 헬기 수준 굉음 고통 호소업체 결함 아냐      4\n",
       "2         작년에 산 롱패딩이 무색하게올 겨울 왜 이상고온일까      1\n",
       "3           독도서 첫 블랙 워터 다이빙한밤 수중 생물 조사      1\n",
       "4      흔들리는 무기 빅딜 훈련기 수출 백지화 우려도 박수찬의       3\n",
       "...                                ...    ...\n",
       "11995      군산에 묻어달라던 서양인이 125년 전 남긴 사진      1\n",
       "11996       전광훈 황교안 정치가 아니다아무리 가르쳐도 안돼      1\n",
       "11997   한 명이라도 더 살려야끝까지 현장 지킨 응급의료 버팀목      4\n",
       "11998         단독 의사 접대 자리에서 성폭력임직원은 쉬쉬      4\n",
       "11999              카톡 보고에 즉답젊은 총수시대 왔다      5\n",
       "\n",
       "[12000 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Twitter\n",
    "from konlpy.tag import Kkma\n",
    "tf.reset_default_graph()\n",
    "twitter = Twitter()\n",
    "token = []\n",
    "embeddingmodel = []\n",
    "for i in range(len(data_set)):\n",
    "    label = data_set['label']\n",
    "    sentence = twitter.pos(data_set['news'][i], norm=True, stem=True)\n",
    "    temp = []\n",
    "    temp_embedding = []\n",
    "    all_temp = []\n",
    "    for k in range(len(sentence)):\n",
    "        temp_embedding.append(sentence[k][0])\n",
    "        temp.append(sentence[k][0] + '/' + sentence[k][1])\n",
    "    all_temp.append(temp)\n",
    "    embeddingmodel.append(temp_embedding)\n",
    "    \n",
    "    all_temp.append(label[i])\n",
    "    token.append(all_temp)\n",
    "    \n",
    "embeddingmodel = []\n",
    "for i in range(len(label_set)):\n",
    "    temp_embeddingmodel = []\n",
    "    for k in range(len(token[i][0])):\n",
    "        temp_embeddingmodel.append(token[i][0][k])\n",
    "    embeddingmodel.append(temp_embeddingmodel)\n",
    "embedding  = Word2Vec(embeddingmodel, size = 300, window =5, min_count = 10,\n",
    "                      iter = 5, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def One_hot(data):\n",
    "       \n",
    "    index_dict = {value:index for index,value in enumerate(set(data))}\n",
    "    result = []\n",
    "\n",
    "    for value in data:\n",
    "\n",
    "        one_hot = np.zeros(len(index_dict))\n",
    "        index = index_dict[value]\n",
    "        one_hot[index] = 1\n",
    "        result.append(one_hot)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def Convert2Vec(model_name, doc): \n",
    "    #train_X_ = W2V.Convert2Vec(\"Word2Vec_csv_article.embedding\",train_X)\n",
    "    word_vec = []\n",
    "    model = model_name\n",
    "    for sent in doc:\n",
    "        sub = []\n",
    "        for word in sent:\n",
    "            if word in model.wv.vocab:\n",
    "                sub.append(model.wv[word]) \n",
    "            else:\n",
    "                sub.append(np.random.uniform(-0.25,0.25,300)) \n",
    "        word_vec.append(sub)\n",
    "\n",
    "    return word_vec\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bi_LSTM():\n",
    "    \n",
    "    def __init__(self, lstm_units, num_class, keep_prob):\n",
    "        \n",
    "        self.lstm_units = lstm_units\n",
    "        \n",
    "        with tf.variable_scope('forward', reuse = tf.AUTO_REUSE):\n",
    "            \n",
    "            self.lstm_fw_cell = tf.nn.rnn_cell.LSTMCell(lstm_units, forget_bias=1.0, state_is_tuple=True)\n",
    "            self.lstm_fw_cell = tf.contrib.rnn.DropoutWrapper(self.lstm_fw_cell, output_keep_prob = keep_prob)\n",
    "            \n",
    "        with tf.variable_scope('backward', reuse = tf.AUTO_REUSE):\n",
    "            \n",
    "            self.lstm_bw_cell = tf.nn.rnn_cell.LSTMCell(lstm_units, forget_bias=1.0, state_is_tuple=True)\n",
    "            self.lstm_bw_cell = tf.contrib.rnn.DropoutWrapper(self.lstm_fw_cell, output_keep_prob = keep_prob)\n",
    "        \n",
    "        with tf.variable_scope('Weights', reuse = tf.AUTO_REUSE):\n",
    "           \n",
    "            self.W = tf.get_variable(name=\"W\", shape=[2 * lstm_units, num_class],\n",
    "                                dtype=tf.float32, initializer = tf.contrib.layers.xavier_initializer())\n",
    "            self.b = tf.get_variable(name=\"b\", shape=[num_class], dtype=tf.float32,\n",
    "                                initializer=tf.zeros_initializer())\n",
    "            \n",
    "            \n",
    "    def logits(self, X, W, b, seq_len):\n",
    "        \n",
    "        (output_fw, output_bw), states = tf.nn.bidirectional_dynamic_rnn(self.lstm_fw_cell, self.lstm_bw_cell,dtype=tf.float32,\n",
    "                                                                            inputs = X, sequence_length = seq_len)\n",
    "        ## concat fw, bw final states\n",
    "        outputs = tf.concat([states[0][1], states[1][1]], axis=1)\n",
    "        pred = tf.matmul(outputs, W) + b        \n",
    "        return pred\n",
    "        \n",
    "    def model_build(self, logits, labels, learning_rate = 0.001):\n",
    "        \n",
    "        with tf.variable_scope(\"loss\"):\n",
    "            \n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits , labels = labels)) # Softmax loss\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss) \n",
    "            \n",
    "        return loss, optimizer\n",
    "    \n",
    "    def graph_build(self, avg_loss, avg_acc):\n",
    "        \n",
    "        tf.summary.scalar('Loss', avg_loss)\n",
    "        tf.summary.scalar('Accuracy', avg_acc)\n",
    "        merged = tf.summary.merge_all()\n",
    "        return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Zero_padding(train_batch_X, Batch_size, Maxseq_length, Vector_size):\n",
    "        \n",
    "    zero_pad = np.zeros((Batch_size, Maxseq_length, Vector_size))\n",
    "    for i in range(Batch_size):\n",
    "        zero_pad[i, :np.shape(train_batch_X[i])[0], :np.shape(train_batch_X[i])[1] ] = train_batch_X[i]\n",
    "    return zero_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28000\n",
      "28000\n",
      "12000\n"
     ]
    }
   ],
   "source": [
    "tokens = np.array(token)\n",
    "data_x = tokens[:,0]\n",
    "data_y = tokens[:,1]\n",
    "\n",
    "\n",
    "data_y = One_hot(data_y)\n",
    "data_x = Convert2Vec(embedding, data_x)\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(data_x,data_y,\n",
    "                                                    test_size = 0.3,\n",
    "                                                    random_state = 42)\n",
    "print(len(train_x))\n",
    "print(len(train_y))\n",
    "print(len(test_x))\n",
    "#train_x = train_x.reset_index(drop=True)\n",
    "#train_y = train_y.reset_index(drop=True)\n",
    "#test_x = test_x.reset_index(drop=True)\n",
    "#test_y = test_y.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_x[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001C7D1DD1A08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001C7D1DD1A08>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001C7D1DD1A08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001C7D1DD1A08>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001C7D1DD1A08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001C7D1DD1A08>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001C7D1DD1A08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001C7D1DD1A08>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001C7D1DD1A08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001C7D1DD1A08>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001C7D1DD1A08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000001C7D1DD1A08>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "Start training!\n"
     ]
    }
   ],
   "source": [
    "Batch_size = 32\n",
    "Total_size = len(train_x)\n",
    "Vector_size = 300\n",
    "seq_length = [len(x) for x in train_x]\n",
    "Maxseq_length = max(seq_length)\n",
    "learning_rate = 0.001\n",
    "lstm_units = 128\n",
    "num_class = 5\n",
    "training_epochs = 20\n",
    "keep_prob = 0.75\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None, Maxseq_length, Vector_size])\n",
    "Y = tf.placeholder(tf.float32, shape = [None, num_class])\n",
    "seq_len = tf.placeholder(tf.int32 , shape = [None])\n",
    "\n",
    "BiLSTM = Bi_LSTM(lstm_units, num_class, keep_prob)\n",
    "with tf.variable_scope('loss', reuse = tf.AUTO_REUSE) :\n",
    "    logits = BiLSTM.logits(X, BiLSTM.W, BiLSTM.b, seq_len)\n",
    "    loss, optimizer = BiLSTM.model_build(logits, Y, learning_rate)\n",
    "\n",
    "prediction = tf.nn.softmax(logits)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "total_batch = int(Total_size / Batch_size)\n",
    "\n",
    "print(\"Start training!\")\n",
    "\n",
    "#modelName = \"BiLSTM.model\"\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>미 항공청보잉 결함 알고도 무시보잉 게이트로 번지나</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>이미선 헌법재판관 임명은 국민 개무시 한국당 정부 규탄 집회</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>이지영 강사 학생들 상대로 천효재단 포교 논란</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>정글의 법칙 취식 사과한 대왕조개 100년 사는 멸종위기종</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>단독 파업 312시간 일 않고도임금 보전받는 르노삼성 노조</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>커피 놀이터가 된 잠실커피 잔만 바꿔도 나만의 스타일 완성되죠</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>인천 중학생 추락사 가해학생 4명은 왜 모두 실형을 받았나</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>송중기송혜교 이혼조정신청드라마 남자친구 박보검과 다정한 모</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>16세 정치 입문세계 최연소 총리 타이틀 거머 쥔 쿠르츠</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>연예전문기자가 본 김혜수  빚 갚다 전재산 잃고 월세살이도</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     news  label\n",
       "0            미 항공청보잉 결함 알고도 무시보잉 게이트로 번지나      2\n",
       "1       이미선 헌법재판관 임명은 국민 개무시 한국당 정부 규탄 집회      3\n",
       "2               이지영 강사 학생들 상대로 천효재단 포교 논란      1\n",
       "3        정글의 법칙 취식 사과한 대왕조개 100년 사는 멸종위기종      2\n",
       "4        단독 파업 312시간 일 않고도임금 보전받는 르노삼성 노조      5\n",
       "...                                   ...    ...\n",
       "11995  커피 놀이터가 된 잠실커피 잔만 바꿔도 나만의 스타일 완성되죠      1\n",
       "11996    인천 중학생 추락사 가해학생 4명은 왜 모두 실형을 받았나      4\n",
       "11997    송중기송혜교 이혼조정신청드라마 남자친구 박보검과 다정한 모      4\n",
       "11998     16세 정치 입문세계 최연소 총리 타이틀 거머 쥔 쿠르츠      2\n",
       "11999    연예전문기자가 본 김혜수  빚 갚다 전재산 잃고 월세살이도      1\n",
       "\n",
       "[12000 rows x 2 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 01 step : 0875 loss = 0.803297 accuracy= 0.625000\n",
      "epoch : 02 step : 0875 loss = 0.761300 accuracy= 0.656250\n",
      "epoch : 03 step : 0875 loss = 0.729546 accuracy= 0.718750\n",
      "epoch : 04 step : 0875 loss = 0.708430 accuracy= 0.718750\n",
      "epoch : 05 step : 0875 loss = 0.674488 accuracy= 0.718750\n",
      "epoch : 06 step : 0875 loss = 0.612798 accuracy= 0.750000\n",
      "epoch : 07 step : 0875 loss = 0.544965 accuracy= 0.843750\n",
      "epoch : 08 step : 0875 loss = 0.473899 accuracy= 0.875000\n",
      "epoch : 09 step : 0875 loss = 0.389920 accuracy= 0.875000\n",
      "epoch : 10 step : 0875 loss = 0.310277 accuracy= 0.906250\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "result  = []\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    start_time = time.time()\n",
    "    sess.run(init)\n",
    "    train_writer = tf.summary.FileWriter('Bidirectional_LSTM', sess.graph)\n",
    "    i = 0\n",
    "    for epoch in range(training_epochs):\n",
    "        #try :\n",
    "            avg_acc, avg_loss = 0. , 0.\n",
    "            for step in range(total_batch):\n",
    "\n",
    "                train_batch_X = train_x[step*Batch_size : step*Batch_size+Batch_size]\n",
    "\n",
    "                train_batch_Y = train_y[step*Batch_size : step*Batch_size+Batch_size]\n",
    "                batch_seq_length = seq_length[step*Batch_size : step*Batch_size+Batch_size]\n",
    "                train_batch_X = Zero_padding(train_batch_X, Batch_size, Maxseq_length, Vector_size)\n",
    "\n",
    "                sess.run(optimizer, feed_dict={X: train_batch_X, Y: train_batch_Y, seq_len: batch_seq_length})\n",
    "                \n",
    "                loss_ = sess.run(loss, feed_dict={X: train_batch_X, Y: train_batch_Y, seq_len: batch_seq_length})\n",
    "                avg_loss += loss_ / total_batch\n",
    "\n",
    "                acc = sess.run(accuracy , feed_dict={X: train_batch_X, Y: train_batch_Y, seq_len: batch_seq_length})\n",
    "                avg_acc += acc / total_batch\n",
    "            print(\"epoch : {:02d} step : {:04d} loss = {:.6f} accuracy= {:.6f}\".format(epoch+1, step+1, loss_, acc))\n",
    "            #print(\"Test Accuracy : \", sess.run(accuracy, feed_dict={X:test_x, Y:test_y}))\n",
    "            summary = sess.run(BiLSTM.graph_build(avg_loss, avg_acc))       \n",
    "            train_writer.add_summary(summary, i)\n",
    "            i += 1\n",
    "        #except : \n",
    "            #print('Error Epoch : ', epoch)\n",
    "    duration = time.time() - start_time\n",
    "    minute = int(duration / 60)\n",
    "    second = int(duration) % 60\n",
    "    print(\"%dminutes %dseconds\" % (minute,second))\n",
    "    save_path = saver.save(sess, os.getcwd())\n",
    "\n",
    "    train_writer.close()\n",
    "    print('save_path',save_path)\n",
    "    \n",
    "    test_size = len(test_x)\n",
    "    test_batch = int(test_size / Batch_size)\n",
    "    seq_length = [len(x) for x in test_x]\n",
    "    keep_prob = 1.0\n",
    "    total_acc = 0\n",
    "    for step in range(test_batch):\n",
    "        test_batch_X = test_x[step*Batch_size: step*Batch_size + Batch_size]\n",
    "        test_batch_Y = test_y[step*Batch_size: step*Batch_size + Batch_size]\n",
    "        batch_seq_length = seq_length[step*Batch_size : step*Batch_size + Batch_size]\n",
    "        test_batch_X = Zero_padding(test_batch_X, Batch_size, Maxseq_length, Vector_size)\n",
    "        \n",
    "        #summary, _ = sess.run([mreged, optimizer], feed_dict = {X: test_batch_X, Y: test_batch_Y, seq_len: batch_seq_length})\n",
    "        acc = sess.run(accuracy , feed_dict={X: test_batch_X, Y: test_batch_Y, seq_len: batch_seq_length})\n",
    "        total_acc += acc/test_batch\n",
    "        pred_v = sess.run(prediction , feed_dict={X: test_batch_X, Y: test_batch_Y, seq_len: batch_seq_length})\n",
    "    #print('acc : ', acc)\n",
    "        \n",
    "        if pred_v is None :\n",
    "                    pred_v = np.zeros(shape=(32,2))\n",
    "        \n",
    "        X_batch = test_x[step*Batch_size : step*Batch_size+Batch_size]\n",
    "        pred_df_row = pd.DataFrame(list(zip(X_batch, test_batch_Y, pred_v)),columns = [\"test_X\",'test_Y','pred'])\n",
    "                \n",
    "        if step == 0:\n",
    "            pred_df = pred_df_row\n",
    "        else:\n",
    "            pred_df = pd.concat([pred_df, pred_df_row], axis = 0)\n",
    "            \n",
    "        if (step > 10 and step & 100 == 0) or (step == test_batch - 1):\n",
    "            loss_ = sess.run(loss, feed_dict ={X: test_batch_X, Y: test_batch_Y, seq_len: batch_seq_length})\n",
    "            acc = sess.run(accuracy , feed_dict = {X:test_batch_X, Y:test_batch_Y, seq_len: batch_seq_length})\n",
    "            \n",
    "            #time_str = datetime.datetime.new().isoforamt()\n",
    "            print(\"loss = {:.6f} accuracy = {:.6f}\".format(loss_, acc), \"/ step =\",step)\n",
    "            #test_writer.add_summary(summary, step)\n",
    "            \n",
    "    print(\"\\nepoch : \", epoch + 1, \" - Total Accuracy : {}\".format(total_acc))\n",
    "    result.append(total_acc)\n",
    "\n",
    "    #pred_df_file = out_dir + \"/pred_value_\" + str(epoch + 1)+'.csv'\n",
    "    #pred_df.to_csv(pred_df_file, index=False, header=False)\n",
    "    #print(\"epoch : \",epoch+1,\"- Saved pred_value : {}\\n\".format(pred_df_file))\n",
    "        \n",
    "\n",
    "print(\"----------------------------------------\")\n",
    "print(\"????????\")\n",
    "for i in range(0, len(result)):\n",
    "    print(\"epoch : \", i+1, \" / accuracy : \", result[i])\n",
    "    \n",
    "print(\"\\nBest Result epoch : \", result.index(max(result)) + 1, \" / accuracy: \", max(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zero_padding(train_batch_X, Batch_size, Maxseq_length, Vector_size)\n",
    "#print(train_batch_X[1].shape)\n",
    "zero_pad = np.zeros((Batch_size, Maxseq_length, Vector_size))\n",
    "print(zero_pad.shape)\n",
    "print(np.shape(train_batch_X[0]))\n",
    "\n",
    "#print(zero_pad.shape[0,list(np.shape(train_batch_X[0]))[0],:])\n",
    "a,b = 0,0\n",
    "for i in range(Batch_size):\n",
    "    a= int(np.shape(train_batch_X[i])[0])\n",
    "    b= int(np.shape(train_batch_X[i])[1])\n",
    "    zero_pad[i,:a,:b] = train_batch_X[i]\n",
    "zero_pad[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(train_batch_X[1])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(train_batch_X[0]).shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(np.shape(train_batch_X[0])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(list(zero_pad.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(np.shape(train_batch_X[0])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(list(np.shape(train_batch_X[0]))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_pad[0,:12,:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(train_batch_X[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_temp[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set['news']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_label = {1:'culture',2:'global',3:'politic',4:'society',5:'economy'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkm = Kkma()\n",
    "kkm.pos(data_set['news'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data_set)):\n",
    "    twitter.pos(data_set['news'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "konl = Twitter()\n",
    "test_string =[\n",
    "\n",
    "'test 데이터를 얼마나 잘 분류하는지 확인합니다.',\n",
    "'학습이 epoch 단위로 진행될 때 예측력이 얼마나 상승하는지 ',\n",
    "'loss에 대한 그래프를 그려서 확인합니다.',\n",
    "'밑에 코드들은 그래프를 이쁘게 그려주기 위한 코드 입니다.'\n",
    "]\n",
    "for row in test_string:\n",
    "    r = konl.pos(row, norm=True, stem=True)\n",
    "    print( '=' * 20)\n",
    "    for txt, post in r:\n",
    "        print( txt, post)\n",
    "    print( '=' * 20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding: utf-8\n",
    "import csv\n",
    "import logging\n",
    "\n",
    "from konlpy import jvm\n",
    "from konlpy.tag import Twitter\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "        konlpy 사용시 주의 사항\n",
    "        자바 설치 및 세팅 필요\n",
    "        JAVA_HOME 세팅이 필요합니다.\n",
    "        export JAVA_HOME=$(/usr/libexec/java_home)\n",
    "    \"\"\"\n",
    "    konl = Twitter()\n",
    "    test_string = [\n",
    "        u'konlpy 사용시 주의 사항',\n",
    "        u'자바 설치 및 세팅 필요',\n",
    "        u'JAVA_HOME 세팅이 필요합니다.',\n",
    "        u'export JAVA_HOME=$(/usr/libexec/java_home)',\n",
    "    ]\n",
    "    for row in test_string:\n",
    "        r = konl.pos(row, norm=True, stem=True)\n",
    "        print( '=' * 20)\n",
    "        for txt, post in r:\n",
    "            print( txt, post)\n",
    "        print( '=' * 20)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    jvm.init_jvm()\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = Word2Vec(data_set['news'], min_count=3, iter=20, size=300,sg=1,\n",
    "                window = 5)\n",
    "model.init_sims(replace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
