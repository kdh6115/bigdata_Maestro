{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 숙제\n",
    "# iris.csv 데이터를 로딩한 다음\n",
    "# 분류망을 구성하세요\n",
    "# parameter tuning을 구현하세요( pipeline도 함께 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3  4\n",
       "1  6.4  2.8  5.6  2.2  2\n",
       "2  5.0  2.3  3.3    1  1\n",
       "3  4.9  2.5  4.5  1.7  2\n",
       "4  4.9  3.1  1.5  0.1  0\n",
       "5  5.7  3.8  1.7  0.3  0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "dataset = pd.read_csv('iris.csv', header=None)\n",
    "dataset.head()\n",
    "dataset = dataset.iloc[1:,:]\n",
    "print(dataset.shape)\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\python\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\python\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\python\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\python\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\python\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\python\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\envs\\tf_test\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\python\\envs\\tf_test\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\python\\envs\\tf_test\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\python\\envs\\tf_test\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\python\\envs\\tf_test\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\python\\envs\\tf_test\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.utils import np_utils\n",
    "X  = dataset.iloc[:,:4]\n",
    "Y = dataset.iloc[:,4]\n",
    "Y = np.asarray(Y)\n",
    "Y = keras.utils.to_categorical(Y)\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\happy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\happy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\happy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\happy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\happy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\happy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\happy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Train on 105 samples, validate on 45 samples\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - 2s 16ms/step - loss: 2.4949 - acc: 0.3048 - val_loss: 2.3062 - val_acc: 0.4222\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - 0s 418us/step - loss: 2.0849 - acc: 0.4952 - val_loss: 2.0334 - val_acc: 0.6000\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - 0s 542us/step - loss: 1.7706 - acc: 0.5238 - val_loss: 1.8474 - val_acc: 0.2889\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - 0s 285us/step - loss: 1.5778 - acc: 0.3905 - val_loss: 1.6677 - val_acc: 0.2222\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - 0s 370us/step - loss: 1.4148 - acc: 0.3810 - val_loss: 1.4696 - val_acc: 0.2222\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - 0s 722us/step - loss: 1.2148 - acc: 0.3810 - val_loss: 1.3002 - val_acc: 0.2222\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - 0s 617us/step - loss: 1.0822 - acc: 0.3810 - val_loss: 1.1386 - val_acc: 0.2444\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - 0s 579us/step - loss: 0.9523 - acc: 0.3905 - val_loss: 1.0188 - val_acc: 0.2444\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - 0s 731us/step - loss: 0.8601 - acc: 0.3905 - val_loss: 0.9291 - val_acc: 0.2667\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - 0s 503us/step - loss: 0.7975 - acc: 0.3905 - val_loss: 0.8737 - val_acc: 0.2667\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - 0s 788us/step - loss: 0.7593 - acc: 0.4000 - val_loss: 0.8362 - val_acc: 0.3111\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - 0s 513us/step - loss: 0.7311 - acc: 0.5524 - val_loss: 0.8097 - val_acc: 0.5333\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - 0s 427us/step - loss: 0.7085 - acc: 0.6476 - val_loss: 0.7848 - val_acc: 0.6000\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - 0s 313us/step - loss: 0.6908 - acc: 0.6952 - val_loss: 0.7670 - val_acc: 0.6000\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - 0s 541us/step - loss: 0.6730 - acc: 0.7048 - val_loss: 0.7425 - val_acc: 0.6222\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - 0s 323us/step - loss: 0.6551 - acc: 0.7333 - val_loss: 0.7304 - val_acc: 0.6000\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - 0s 437us/step - loss: 0.6386 - acc: 0.7143 - val_loss: 0.7168 - val_acc: 0.6222\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - 0s 485us/step - loss: 0.6245 - acc: 0.7143 - val_loss: 0.6932 - val_acc: 0.6444\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - 0s 525us/step - loss: 0.6100 - acc: 0.7333 - val_loss: 0.6787 - val_acc: 0.6444\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - 0s 579us/step - loss: 0.5965 - acc: 0.7333 - val_loss: 0.6658 - val_acc: 0.6444\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - 0s 456us/step - loss: 0.5849 - acc: 0.7333 - val_loss: 0.6604 - val_acc: 0.6222\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - 0s 487us/step - loss: 0.5742 - acc: 0.7333 - val_loss: 0.6342 - val_acc: 0.7333\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - 0s 475us/step - loss: 0.5628 - acc: 0.7714 - val_loss: 0.6229 - val_acc: 0.7333\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - 0s 532us/step - loss: 0.5523 - acc: 0.7333 - val_loss: 0.6190 - val_acc: 0.6444\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - 0s 391us/step - loss: 0.5419 - acc: 0.7619 - val_loss: 0.5985 - val_acc: 0.7556\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - 0s 515us/step - loss: 0.5330 - acc: 0.8190 - val_loss: 0.5867 - val_acc: 0.7778\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - 0s 465us/step - loss: 0.5253 - acc: 0.8286 - val_loss: 0.5744 - val_acc: 0.8000\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - 0s 418us/step - loss: 0.5180 - acc: 0.8381 - val_loss: 0.5625 - val_acc: 0.8444\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - 0s 475us/step - loss: 0.5094 - acc: 0.8381 - val_loss: 0.5613 - val_acc: 0.7778\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - 0s 608us/step - loss: 0.5032 - acc: 0.7905 - val_loss: 0.5572 - val_acc: 0.7556\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - 0s 475us/step - loss: 0.4952 - acc: 0.8000 - val_loss: 0.5485 - val_acc: 0.7778\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - 0s 285us/step - loss: 0.4897 - acc: 0.8000 - val_loss: 0.5462 - val_acc: 0.7556\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - 0s 370us/step - loss: 0.4833 - acc: 0.8095 - val_loss: 0.5316 - val_acc: 0.8000\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - 0s 351us/step - loss: 0.4794 - acc: 0.8476 - val_loss: 0.5166 - val_acc: 0.8889\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - 0s 304us/step - loss: 0.4708 - acc: 0.8381 - val_loss: 0.5199 - val_acc: 0.8000\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - 0s 315us/step - loss: 0.4664 - acc: 0.8381 - val_loss: 0.5196 - val_acc: 0.7556\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - 0s 246us/step - loss: 0.4613 - acc: 0.8381 - val_loss: 0.5057 - val_acc: 0.8444\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - 0s 351us/step - loss: 0.4558 - acc: 0.8381 - val_loss: 0.4957 - val_acc: 0.8889\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - 0s 361us/step - loss: 0.4514 - acc: 0.8667 - val_loss: 0.4879 - val_acc: 0.8889\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - 0s 465us/step - loss: 0.4476 - acc: 0.8571 - val_loss: 0.4872 - val_acc: 0.8667\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - 0s 399us/step - loss: 0.4425 - acc: 0.8381 - val_loss: 0.4899 - val_acc: 0.8222\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - 0s 389us/step - loss: 0.4398 - acc: 0.8476 - val_loss: 0.4796 - val_acc: 0.8667\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - 0s 370us/step - loss: 0.4332 - acc: 0.8381 - val_loss: 0.4731 - val_acc: 0.8889\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - 0s 630us/step - loss: 0.4289 - acc: 0.8667 - val_loss: 0.4656 - val_acc: 0.8889\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - 0s 256us/step - loss: 0.4249 - acc: 0.8667 - val_loss: 0.4625 - val_acc: 0.8889\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - 0s 640us/step - loss: 0.4211 - acc: 0.8762 - val_loss: 0.4560 - val_acc: 0.8889\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - 0s 475us/step - loss: 0.4170 - acc: 0.8762 - val_loss: 0.4546 - val_acc: 0.8889\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - 0s 433us/step - loss: 0.4133 - acc: 0.8571 - val_loss: 0.4517 - val_acc: 0.8889\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - 0s 936us/step - loss: 0.4091 - acc: 0.8667 - val_loss: 0.4445 - val_acc: 0.8889\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - 0s 408us/step - loss: 0.4068 - acc: 0.8952 - val_loss: 0.4304 - val_acc: 0.9333\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - 0s 342us/step - loss: 0.4028 - acc: 0.9429 - val_loss: 0.4322 - val_acc: 0.8889\n",
      "Epoch 52/100\n",
      "105/105 [==============================] - 0s 322us/step - loss: 0.3991 - acc: 0.8762 - val_loss: 0.4349 - val_acc: 0.8889\n",
      "Epoch 53/100\n",
      "105/105 [==============================] - 0s 322us/step - loss: 0.3950 - acc: 0.8952 - val_loss: 0.4217 - val_acc: 0.9111\n",
      "Epoch 54/100\n",
      "105/105 [==============================] - 0s 285us/step - loss: 0.3920 - acc: 0.9238 - val_loss: 0.4195 - val_acc: 0.9111\n",
      "Epoch 55/100\n",
      "105/105 [==============================] - 0s 326us/step - loss: 0.3885 - acc: 0.8952 - val_loss: 0.4179 - val_acc: 0.8889\n",
      "Epoch 56/100\n",
      "105/105 [==============================] - 0s 342us/step - loss: 0.3851 - acc: 0.9333 - val_loss: 0.4033 - val_acc: 0.9556\n",
      "Epoch 57/100\n",
      "105/105 [==============================] - 0s 551us/step - loss: 0.3823 - acc: 0.9429 - val_loss: 0.4045 - val_acc: 0.9333\n",
      "Epoch 58/100\n",
      "105/105 [==============================] - 0s 437us/step - loss: 0.3782 - acc: 0.9333 - val_loss: 0.3989 - val_acc: 0.9333\n",
      "Epoch 59/100\n",
      "105/105 [==============================] - 0s 570us/step - loss: 0.3761 - acc: 0.9333 - val_loss: 0.4007 - val_acc: 0.9111\n",
      "Epoch 60/100\n",
      "105/105 [==============================] - 0s 399us/step - loss: 0.3720 - acc: 0.9429 - val_loss: 0.3909 - val_acc: 0.9333\n",
      "Epoch 61/100\n",
      "105/105 [==============================] - 0s 512us/step - loss: 0.3721 - acc: 0.9238 - val_loss: 0.3943 - val_acc: 0.9111\n",
      "Epoch 62/100\n",
      "105/105 [==============================] - 0s 521us/step - loss: 0.3644 - acc: 0.9333 - val_loss: 0.3817 - val_acc: 0.9556\n",
      "Epoch 63/100\n",
      "105/105 [==============================] - 0s 522us/step - loss: 0.3632 - acc: 0.9524 - val_loss: 0.3769 - val_acc: 0.9556\n",
      "Epoch 64/100\n",
      "105/105 [==============================] - 0s 432us/step - loss: 0.3600 - acc: 0.9524 - val_loss: 0.3747 - val_acc: 0.9556\n",
      "Epoch 65/100\n",
      "105/105 [==============================] - 0s 465us/step - loss: 0.3557 - acc: 0.9429 - val_loss: 0.3750 - val_acc: 0.9333\n",
      "Epoch 66/100\n",
      "105/105 [==============================] - 0s 437us/step - loss: 0.3523 - acc: 0.9429 - val_loss: 0.3719 - val_acc: 0.9333\n",
      "Epoch 67/100\n",
      "105/105 [==============================] - 0s 354us/step - loss: 0.3493 - acc: 0.9524 - val_loss: 0.3603 - val_acc: 0.9778\n",
      "Epoch 68/100\n",
      "105/105 [==============================] - 0s 402us/step - loss: 0.3464 - acc: 0.9619 - val_loss: 0.3567 - val_acc: 0.9778\n",
      "Epoch 69/100\n",
      "105/105 [==============================] - 0s 437us/step - loss: 0.3433 - acc: 0.9524 - val_loss: 0.3584 - val_acc: 0.9556\n",
      "Epoch 70/100\n",
      "105/105 [==============================] - 0s 467us/step - loss: 0.3422 - acc: 0.9333 - val_loss: 0.3675 - val_acc: 0.9111\n",
      "Epoch 71/100\n",
      "105/105 [==============================] - 0s 475us/step - loss: 0.3394 - acc: 0.9429 - val_loss: 0.3441 - val_acc: 0.9778\n",
      "Epoch 72/100\n",
      "105/105 [==============================] - 0s 495us/step - loss: 0.3360 - acc: 0.9619 - val_loss: 0.3456 - val_acc: 0.9778\n",
      "Epoch 73/100\n",
      "105/105 [==============================] - 0s 465us/step - loss: 0.3312 - acc: 0.9524 - val_loss: 0.3412 - val_acc: 0.9778\n",
      "Epoch 74/100\n",
      "105/105 [==============================] - 0s 507us/step - loss: 0.3286 - acc: 0.9524 - val_loss: 0.3444 - val_acc: 0.9778\n",
      "Epoch 75/100\n",
      "105/105 [==============================] - 0s 575us/step - loss: 0.3261 - acc: 0.9524 - val_loss: 0.3375 - val_acc: 0.9778\n",
      "Epoch 76/100\n",
      "105/105 [==============================] - 0s 798us/step - loss: 0.3234 - acc: 0.9524 - val_loss: 0.3377 - val_acc: 0.9778\n",
      "Epoch 77/100\n",
      "105/105 [==============================] - 0s 560us/step - loss: 0.3200 - acc: 0.9524 - val_loss: 0.3314 - val_acc: 0.9778\n",
      "Epoch 78/100\n",
      "105/105 [==============================] - 0s 566us/step - loss: 0.3180 - acc: 0.9524 - val_loss: 0.3303 - val_acc: 0.9778\n",
      "Epoch 79/100\n",
      "105/105 [==============================] - 0s 598us/step - loss: 0.3157 - acc: 0.9524 - val_loss: 0.3250 - val_acc: 0.9778\n",
      "Epoch 80/100\n",
      "105/105 [==============================] - 0s 486us/step - loss: 0.3129 - acc: 0.9524 - val_loss: 0.3252 - val_acc: 0.9778\n",
      "Epoch 81/100\n",
      "105/105 [==============================] - 0s 389us/step - loss: 0.3151 - acc: 0.9333 - val_loss: 0.3323 - val_acc: 0.9333\n",
      "Epoch 82/100\n",
      "105/105 [==============================] - 0s 351us/step - loss: 0.3090 - acc: 0.9429 - val_loss: 0.3144 - val_acc: 0.9778\n",
      "Epoch 83/100\n",
      "105/105 [==============================] - 0s 969us/step - loss: 0.3051 - acc: 0.9619 - val_loss: 0.3105 - val_acc: 0.9778\n",
      "Epoch 84/100\n",
      "105/105 [==============================] - 0s 836us/step - loss: 0.3016 - acc: 0.9619 - val_loss: 0.3118 - val_acc: 0.9778\n",
      "Epoch 85/100\n",
      "105/105 [==============================] - 0s 510us/step - loss: 0.2994 - acc: 0.9524 - val_loss: 0.3117 - val_acc: 0.9778\n",
      "Epoch 86/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.3032 - acc: 0.960 - 0s 876us/step - loss: 0.2975 - acc: 0.9619 - val_loss: 0.3015 - val_acc: 0.9778\n",
      "Epoch 87/100\n",
      "105/105 [==============================] - 0s 608us/step - loss: 0.2956 - acc: 0.9524 - val_loss: 0.3052 - val_acc: 0.9778\n",
      "Epoch 88/100\n",
      "105/105 [==============================] - 0s 257us/step - loss: 0.2931 - acc: 0.9524 - val_loss: 0.2975 - val_acc: 0.9778\n",
      "Epoch 89/100\n",
      "105/105 [==============================] - 0s 361us/step - loss: 0.2904 - acc: 0.9619 - val_loss: 0.2962 - val_acc: 0.9778\n",
      "Epoch 90/100\n",
      "105/105 [==============================] - 0s 380us/step - loss: 0.2877 - acc: 0.9714 - val_loss: 0.2919 - val_acc: 0.9778\n",
      "Epoch 91/100\n",
      "105/105 [==============================] - 0s 317us/step - loss: 0.2866 - acc: 0.9714 - val_loss: 0.2840 - val_acc: 0.9778\n",
      "Epoch 92/100\n",
      "105/105 [==============================] - 0s 494us/step - loss: 0.2839 - acc: 0.9714 - val_loss: 0.2832 - val_acc: 0.9778\n",
      "Epoch 93/100\n",
      "105/105 [==============================] - 0s 399us/step - loss: 0.2808 - acc: 0.9619 - val_loss: 0.2863 - val_acc: 0.9778\n",
      "Epoch 94/100\n",
      "105/105 [==============================] - 0s 390us/step - loss: 0.2789 - acc: 0.9524 - val_loss: 0.2877 - val_acc: 0.9778\n",
      "Epoch 95/100\n",
      "105/105 [==============================] - 0s 530us/step - loss: 0.2775 - acc: 0.9524 - val_loss: 0.2782 - val_acc: 0.9778\n",
      "Epoch 96/100\n",
      "105/105 [==============================] - 0s 475us/step - loss: 0.2750 - acc: 0.9619 - val_loss: 0.2823 - val_acc: 0.9778\n",
      "Epoch 97/100\n",
      "105/105 [==============================] - 0s 632us/step - loss: 0.2727 - acc: 0.9524 - val_loss: 0.2790 - val_acc: 0.9778\n",
      "Epoch 98/100\n",
      "105/105 [==============================] - 0s 513us/step - loss: 0.2704 - acc: 0.9714 - val_loss: 0.2680 - val_acc: 0.9778\n",
      "Epoch 99/100\n",
      "105/105 [==============================] - 0s 448us/step - loss: 0.2684 - acc: 0.9714 - val_loss: 0.2631 - val_acc: 0.9778\n",
      "Epoch 100/100\n",
      "105/105 [==============================] - 0s 416us/step - loss: 0.2676 - acc: 0.9714 - val_loss: 0.2661 - val_acc: 0.9778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1956d0f2cc8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(15, input_dim = 4, activation = 'relu') )\n",
    "model.add(Dense(3, activation  = 'softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.fit(X,Y,epochs=100, batch_size=10, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 실행문\n",
    "- gridsearchcv 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model() :\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=4, kernel_initializer='normal', activation = 'relu'))\n",
    "    model.add(Dense(3, activation = 'softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "150/150 [==============================] - 1s 8ms/step - loss: 1.0709 - acc: 0.3333\n",
      "Epoch 2/100\n",
      "150/150 [==============================] - 0s 253us/step - loss: 1.0484 - acc: 0.3333\n",
      "Epoch 3/100\n",
      "150/150 [==============================] - 0s 288us/step - loss: 1.0288 - acc: 0.3733\n",
      "Epoch 4/100\n",
      "150/150 [==============================] - 0s 293us/step - loss: 1.0104 - acc: 0.5733\n",
      "Epoch 5/100\n",
      "150/150 [==============================] - 0s 267us/step - loss: 0.9921 - acc: 0.6200\n",
      "Epoch 6/100\n",
      "150/150 [==============================] - 0s 352us/step - loss: 0.9736 - acc: 0.6667\n",
      "Epoch 7/100\n",
      "150/150 [==============================] - 0s 232us/step - loss: 0.9538 - acc: 0.6667\n",
      "Epoch 8/100\n",
      "150/150 [==============================] - 0s 266us/step - loss: 0.9334 - acc: 0.6667\n",
      "Epoch 9/100\n",
      "150/150 [==============================] - 0s 492us/step - loss: 0.9132 - acc: 0.5467\n",
      "Epoch 10/100\n",
      "150/150 [==============================] - 0s 246us/step - loss: 0.8887 - acc: 0.5533\n",
      "Epoch 11/100\n",
      "150/150 [==============================] - 0s 266us/step - loss: 0.8659 - acc: 0.6067\n",
      "Epoch 12/100\n",
      "150/150 [==============================] - 0s 246us/step - loss: 0.8425 - acc: 0.6667\n",
      "Epoch 13/100\n",
      "150/150 [==============================] - 0s 293us/step - loss: 0.8166 - acc: 0.6667\n",
      "Epoch 14/100\n",
      "150/150 [==============================] - 0s 248us/step - loss: 0.7920 - acc: 0.6667\n",
      "Epoch 15/100\n",
      "150/150 [==============================] - 0s 265us/step - loss: 0.7689 - acc: 0.6667\n",
      "Epoch 16/100\n",
      "150/150 [==============================] - 0s 351us/step - loss: 0.7464 - acc: 0.6667\n",
      "Epoch 17/100\n",
      "150/150 [==============================] - 0s 273us/step - loss: 0.7222 - acc: 0.6667\n",
      "Epoch 18/100\n",
      "150/150 [==============================] - 0s 293us/step - loss: 0.7018 - acc: 0.6667\n",
      "Epoch 19/100\n",
      "150/150 [==============================] - 0s 239us/step - loss: 0.6800 - acc: 0.6667\n",
      "Epoch 20/100\n",
      "150/150 [==============================] - 0s 339us/step - loss: 0.6611 - acc: 0.6667\n",
      "Epoch 21/100\n",
      "150/150 [==============================] - 0s 245us/step - loss: 0.6423 - acc: 0.6667\n",
      "Epoch 22/100\n",
      "150/150 [==============================] - 0s 240us/step - loss: 0.6239 - acc: 0.6667\n",
      "Epoch 23/100\n",
      "150/150 [==============================] - 0s 259us/step - loss: 0.6087 - acc: 0.6667\n",
      "Epoch 24/100\n",
      "150/150 [==============================] - 0s 314us/step - loss: 0.5921 - acc: 0.6667\n",
      "Epoch 25/100\n",
      "150/150 [==============================] - 0s 233us/step - loss: 0.5776 - acc: 0.6800\n",
      "Epoch 26/100\n",
      "150/150 [==============================] - 0s 239us/step - loss: 0.5649 - acc: 0.6800\n",
      "Epoch 27/100\n",
      "150/150 [==============================] - 0s 366us/step - loss: 0.5516 - acc: 0.6800\n",
      "Epoch 28/100\n",
      "150/150 [==============================] - 0s 273us/step - loss: 0.5399 - acc: 0.6933\n",
      "Epoch 29/100\n",
      "150/150 [==============================] - 0s 253us/step - loss: 0.5293 - acc: 0.7133\n",
      "Epoch 30/100\n",
      "150/150 [==============================] - 0s 259us/step - loss: 0.5200 - acc: 0.7133\n",
      "Epoch 31/100\n",
      "150/150 [==============================] - 0s 286us/step - loss: 0.5091 - acc: 0.7333\n",
      "Epoch 32/100\n",
      "150/150 [==============================] - 0s 339us/step - loss: 0.5010 - acc: 0.7400\n",
      "Epoch 33/100\n",
      "150/150 [==============================] - 0s 273us/step - loss: 0.4915 - acc: 0.7933\n",
      "Epoch 34/100\n",
      "150/150 [==============================] - 0s 253us/step - loss: 0.4832 - acc: 0.8000\n",
      "Epoch 35/100\n",
      "150/150 [==============================] - 0s 253us/step - loss: 0.4755 - acc: 0.8333\n",
      "Epoch 36/100\n",
      "150/150 [==============================] - 0s 246us/step - loss: 0.4683 - acc: 0.8867\n",
      "Epoch 37/100\n",
      "150/150 [==============================] - 0s 259us/step - loss: 0.4611 - acc: 0.8333\n",
      "Epoch 38/100\n",
      "150/150 [==============================] - 0s 359us/step - loss: 0.4547 - acc: 0.8200\n",
      "Epoch 39/100\n",
      "150/150 [==============================] - 0s 299us/step - loss: 0.4488 - acc: 0.8933\n",
      "Epoch 40/100\n",
      "150/150 [==============================] - 0s 233us/step - loss: 0.4403 - acc: 0.8933\n",
      "Epoch 41/100\n",
      "150/150 [==============================] - 0s 226us/step - loss: 0.4339 - acc: 0.9067\n",
      "Epoch 42/100\n",
      "150/150 [==============================] - 0s 259us/step - loss: 0.4280 - acc: 0.9267\n",
      "Epoch 43/100\n",
      "150/150 [==============================] - 0s 360us/step - loss: 0.4220 - acc: 0.9133\n",
      "Epoch 44/100\n",
      "150/150 [==============================] - 0s 339us/step - loss: 0.4159 - acc: 0.9400\n",
      "Epoch 45/100\n",
      "150/150 [==============================] - 0s 359us/step - loss: 0.4110 - acc: 0.9467\n",
      "Epoch 46/100\n",
      "150/150 [==============================] - 0s 293us/step - loss: 0.4051 - acc: 0.9467\n",
      "Epoch 47/100\n",
      "150/150 [==============================] - 0s 246us/step - loss: 0.4005 - acc: 0.9533\n",
      "Epoch 48/100\n",
      "150/150 [==============================] - 0s 246us/step - loss: 0.3939 - acc: 0.9533\n",
      "Epoch 49/100\n",
      "150/150 [==============================] - 0s 299us/step - loss: 0.3896 - acc: 0.9467\n",
      "Epoch 50/100\n",
      "150/150 [==============================] - 0s 239us/step - loss: 0.3847 - acc: 0.9467\n",
      "Epoch 51/100\n",
      "150/150 [==============================] - 0s 279us/step - loss: 0.3798 - acc: 0.9600\n",
      "Epoch 52/100\n",
      "150/150 [==============================] - 0s 253us/step - loss: 0.3749 - acc: 0.9600\n",
      "Epoch 53/100\n",
      "150/150 [==============================] - 0s 448us/step - loss: 0.3698 - acc: 0.9600 0s - loss: 0.3655 - acc: 0.961\n",
      "Epoch 54/100\n",
      "150/150 [==============================] - 0s 279us/step - loss: 0.3647 - acc: 0.9600\n",
      "Epoch 55/100\n",
      "150/150 [==============================] - 0s 246us/step - loss: 0.3606 - acc: 0.9533\n",
      "Epoch 56/100\n",
      "150/150 [==============================] - 0s 239us/step - loss: 0.3557 - acc: 0.9600\n",
      "Epoch 57/100\n",
      "150/150 [==============================] - 0s 227us/step - loss: 0.3532 - acc: 0.9600\n",
      "Epoch 58/100\n",
      "150/150 [==============================] - 0s 279us/step - loss: 0.3474 - acc: 0.9600\n",
      "Epoch 59/100\n",
      "150/150 [==============================] - 0s 239us/step - loss: 0.3446 - acc: 0.9800\n",
      "Epoch 60/100\n",
      "150/150 [==============================] - 0s 285us/step - loss: 0.3424 - acc: 0.9533\n",
      "Epoch 61/100\n",
      "150/150 [==============================] - 0s 239us/step - loss: 0.3336 - acc: 0.9600\n",
      "Epoch 62/100\n",
      "150/150 [==============================] - 0s 253us/step - loss: 0.3306 - acc: 0.9667\n",
      "Epoch 63/100\n",
      "150/150 [==============================] - 0s 300us/step - loss: 0.3259 - acc: 0.9733\n",
      "Epoch 64/100\n",
      "150/150 [==============================] - 0s 226us/step - loss: 0.3214 - acc: 0.9600\n",
      "Epoch 65/100\n",
      "150/150 [==============================] - 0s 259us/step - loss: 0.3180 - acc: 0.9667\n",
      "Epoch 66/100\n",
      "150/150 [==============================] - 0s 255us/step - loss: 0.3147 - acc: 0.9667\n",
      "Epoch 67/100\n",
      "150/150 [==============================] - 0s 260us/step - loss: 0.3103 - acc: 0.9667\n",
      "Epoch 68/100\n",
      "150/150 [==============================] - 0s 270us/step - loss: 0.3076 - acc: 0.9600\n",
      "Epoch 69/100\n",
      "150/150 [==============================] - 0s 293us/step - loss: 0.3025 - acc: 0.9667\n",
      "Epoch 70/100\n",
      "150/150 [==============================] - 0s 259us/step - loss: 0.2990 - acc: 0.9733\n",
      "Epoch 71/100\n",
      "150/150 [==============================] - 0s 259us/step - loss: 0.2962 - acc: 0.9667\n",
      "Epoch 72/100\n",
      "150/150 [==============================] - 0s 253us/step - loss: 0.2916 - acc: 0.9733\n",
      "Epoch 73/100\n",
      "150/150 [==============================] - 0s 259us/step - loss: 0.2889 - acc: 0.9800\n",
      "Epoch 74/100\n",
      "150/150 [==============================] - 0s 249us/step - loss: 0.2851 - acc: 0.9733\n",
      "Epoch 75/100\n",
      "150/150 [==============================] - 0s 237us/step - loss: 0.2820 - acc: 0.9667\n",
      "Epoch 76/100\n",
      "150/150 [==============================] - 0s 242us/step - loss: 0.2791 - acc: 0.9667\n",
      "Epoch 77/100\n",
      "150/150 [==============================] - 0s 246us/step - loss: 0.2747 - acc: 0.9733\n",
      "Epoch 78/100\n",
      "150/150 [==============================] - 0s 280us/step - loss: 0.2714 - acc: 0.9733\n",
      "Epoch 79/100\n",
      "150/150 [==============================] - 0s 206us/step - loss: 0.2682 - acc: 0.9733\n",
      "Epoch 80/100\n",
      "150/150 [==============================] - 0s 253us/step - loss: 0.2652 - acc: 0.9733\n",
      "Epoch 81/100\n",
      "150/150 [==============================] - 0s 259us/step - loss: 0.2623 - acc: 0.9733\n",
      "Epoch 82/100\n",
      "150/150 [==============================] - 0s 253us/step - loss: 0.2588 - acc: 0.9733\n",
      "Epoch 83/100\n",
      "150/150 [==============================] - 0s 259us/step - loss: 0.2570 - acc: 0.9733\n",
      "Epoch 84/100\n",
      "150/150 [==============================] - 0s 226us/step - loss: 0.2534 - acc: 0.9733\n",
      "Epoch 85/100\n",
      "150/150 [==============================] - 0s 239us/step - loss: 0.2503 - acc: 0.9733\n",
      "Epoch 86/100\n",
      "150/150 [==============================] - 0s 259us/step - loss: 0.2480 - acc: 0.9733\n",
      "Epoch 87/100\n",
      "150/150 [==============================] - 0s 262us/step - loss: 0.2448 - acc: 0.9733\n",
      "Epoch 88/100\n",
      "150/150 [==============================] - 0s 226us/step - loss: 0.2434 - acc: 0.9733\n",
      "Epoch 89/100\n",
      "150/150 [==============================] - 0s 239us/step - loss: 0.2397 - acc: 0.9733\n",
      "Epoch 90/100\n",
      "150/150 [==============================] - 0s 286us/step - loss: 0.2377 - acc: 0.9733\n",
      "Epoch 91/100\n",
      "150/150 [==============================] - 0s 346us/step - loss: 0.2340 - acc: 0.9733\n",
      "Epoch 92/100\n",
      "150/150 [==============================] - 0s 305us/step - loss: 0.2320 - acc: 0.9733\n",
      "Epoch 93/100\n",
      "150/150 [==============================] - 0s 312us/step - loss: 0.2297 - acc: 0.9733\n",
      "Epoch 94/100\n",
      "150/150 [==============================] - 0s 296us/step - loss: 0.2271 - acc: 0.9800\n",
      "Epoch 95/100\n",
      "150/150 [==============================] - 0s 201us/step - loss: 0.2252 - acc: 0.9800\n",
      "Epoch 96/100\n",
      "150/150 [==============================] - 0s 319us/step - loss: 0.2221 - acc: 0.9733\n",
      "Epoch 97/100\n",
      "150/150 [==============================] - 0s 312us/step - loss: 0.2200 - acc: 0.9733\n",
      "Epoch 98/100\n",
      "150/150 [==============================] - 0s 233us/step - loss: 0.2183 - acc: 0.9733\n",
      "Epoch 99/100\n",
      "150/150 [==============================] - 0s 246us/step - loss: 0.2146 - acc: 0.9667\n",
      "Epoch 100/100\n",
      "150/150 [==============================] - 0s 304us/step - loss: 0.2134 - acc: 0.9733\n",
      "최적스코어 : 0.9200  사용한 파라미터 조합 : {'batch_size': 10, 'epochs': 100}\n",
      "0.39 (0.13)    with {'batch_size': 10, 'epochs': 10}\n",
      "0.85 (0.13)    with {'batch_size': 10, 'epochs': 50}\n",
      "0.92 (0.10)    with {'batch_size': 10, 'epochs': 100}\n",
      "0.39 (0.12)    with {'batch_size': 20, 'epochs': 10}\n",
      "0.71 (0.25)    with {'batch_size': 20, 'epochs': 50}\n",
      "0.91 (0.08)    with {'batch_size': 20, 'epochs': 100}\n",
      "0.37 (0.11)    with {'batch_size': 30, 'epochs': 10}\n",
      "0.60 (0.08)    with {'batch_size': 30, 'epochs': 50}\n",
      "0.76 (0.26)    with {'batch_size': 30, 'epochs': 100}\n"
     ]
    }
   ],
   "source": [
    "model = KerasClassifier(build_fn=baseline_model, verbose=1 )\n",
    "batch_size = [10,20,30]\n",
    "epochs = [10,50,100]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "grid = GridSearchCV(model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X,Y)\n",
    "print(\"최적스코어 : {:.4f}  사용한 파라미터 조합 : {}\".format(grid_result.best_score_,grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, std, param in zip(means, stds, params):\n",
    "    print(\"{:.2f} ({:.2f})    with {}\".format(mean,std,param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ++ 파이프라인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 1.0971 - acc: 0.3867\n",
      "Epoch 2/100\n",
      "150/150 [==============================] - 0s 286us/step - loss: 1.0823 - acc: 0.6600\n",
      "Epoch 3/100\n",
      "150/150 [==============================] - 0s 299us/step - loss: 1.0713 - acc: 0.6600\n",
      "Epoch 4/100\n",
      "150/150 [==============================] - 0s 284us/step - loss: 1.0610 - acc: 0.6667\n",
      "Epoch 5/100\n",
      "150/150 [==============================] - 0s 312us/step - loss: 1.0509 - acc: 0.6667\n",
      "Epoch 6/100\n",
      "150/150 [==============================] - 0s 319us/step - loss: 1.0403 - acc: 0.6667\n",
      "Epoch 7/100\n",
      "150/150 [==============================] - 0s 260us/step - loss: 1.0291 - acc: 0.6667\n",
      "Epoch 8/100\n",
      "150/150 [==============================] - 0s 312us/step - loss: 1.0161 - acc: 0.6667\n",
      "Epoch 9/100\n",
      "150/150 [==============================] - 0s 353us/step - loss: 1.0024 - acc: 0.6667\n",
      "Epoch 10/100\n",
      "150/150 [==============================] - 0s 332us/step - loss: 0.9883 - acc: 0.6667\n",
      "Epoch 11/100\n",
      "150/150 [==============================] - 0s 258us/step - loss: 0.9743 - acc: 0.6667\n",
      "Epoch 12/100\n",
      "150/150 [==============================] - 0s 306us/step - loss: 0.9578 - acc: 0.6667\n",
      "Epoch 13/100\n",
      "150/150 [==============================] - 0s 307us/step - loss: 0.9413 - acc: 0.6667\n",
      "Epoch 14/100\n",
      "150/150 [==============================] - 0s 339us/step - loss: 0.9243 - acc: 0.6667\n",
      "Epoch 15/100\n",
      "150/150 [==============================] - 0s 304us/step - loss: 0.9066 - acc: 0.6667\n",
      "Epoch 16/100\n",
      "150/150 [==============================] - 0s 291us/step - loss: 0.8882 - acc: 0.6667\n",
      "Epoch 17/100\n",
      "150/150 [==============================] - 0s 239us/step - loss: 0.8695 - acc: 0.6667\n",
      "Epoch 18/100\n",
      "150/150 [==============================] - 0s 283us/step - loss: 0.8518 - acc: 0.6667\n",
      "Epoch 19/100\n",
      "150/150 [==============================] - 0s 273us/step - loss: 0.8319 - acc: 0.6667\n",
      "Epoch 20/100\n",
      "150/150 [==============================] - 0s 397us/step - loss: 0.8134 - acc: 0.6667\n",
      "Epoch 21/100\n",
      "150/150 [==============================] - 0s 319us/step - loss: 0.7947 - acc: 0.6667\n",
      "Epoch 22/100\n",
      "150/150 [==============================] - 0s 253us/step - loss: 0.7768 - acc: 0.6667\n",
      "Epoch 23/100\n",
      "150/150 [==============================] - 0s 306us/step - loss: 0.7587 - acc: 0.6667\n",
      "Epoch 24/100\n",
      "150/150 [==============================] - 0s 299us/step - loss: 0.7407 - acc: 0.6667\n",
      "Epoch 25/100\n",
      "150/150 [==============================] - 0s 270us/step - loss: 0.7247 - acc: 0.6667\n",
      "Epoch 26/100\n",
      "150/150 [==============================] - 0s 274us/step - loss: 0.7075 - acc: 0.6667\n",
      "Epoch 27/100\n",
      "150/150 [==============================] - 0s 261us/step - loss: 0.6920 - acc: 0.6667\n",
      "Epoch 28/100\n",
      "150/150 [==============================] - 0s 231us/step - loss: 0.6770 - acc: 0.6667\n",
      "Epoch 29/100\n",
      "150/150 [==============================] - 0s 366us/step - loss: 0.6626 - acc: 0.6667\n",
      "Epoch 30/100\n",
      "150/150 [==============================] - 0s 239us/step - loss: 0.6487 - acc: 0.6667\n",
      "Epoch 31/100\n",
      "150/150 [==============================] - 0s 315us/step - loss: 0.6358 - acc: 0.6667\n",
      "Epoch 32/100\n",
      "150/150 [==============================] - 0s 293us/step - loss: 0.6234 - acc: 0.6667\n",
      "Epoch 33/100\n",
      "150/150 [==============================] - 0s 253us/step - loss: 0.6114 - acc: 0.6667\n",
      "Epoch 34/100\n",
      "150/150 [==============================] - 0s 279us/step - loss: 0.6001 - acc: 0.6800\n",
      "Epoch 35/100\n",
      "150/150 [==============================] - 0s 293us/step - loss: 0.5893 - acc: 0.6867\n",
      "Epoch 36/100\n",
      "150/150 [==============================] - 0s 550us/step - loss: 0.5792 - acc: 0.6867\n",
      "Epoch 37/100\n",
      "150/150 [==============================] - 0s 226us/step - loss: 0.5693 - acc: 0.6867\n",
      "Epoch 38/100\n",
      "150/150 [==============================] - 0s 239us/step - loss: 0.5602 - acc: 0.6867\n",
      "Epoch 39/100\n",
      "150/150 [==============================] - 0s 273us/step - loss: 0.5514 - acc: 0.6867\n",
      "Epoch 40/100\n",
      "150/150 [==============================] - 0s 253us/step - loss: 0.5429 - acc: 0.6933\n",
      "Epoch 41/100\n",
      "150/150 [==============================] - 0s 372us/step - loss: 0.5348 - acc: 0.6933\n",
      "Epoch 42/100\n",
      "150/150 [==============================] - 0s 246us/step - loss: 0.5270 - acc: 0.7067\n",
      "Epoch 43/100\n",
      "150/150 [==============================] - 0s 229us/step - loss: 0.5200 - acc: 0.7067\n",
      "Epoch 44/100\n",
      "150/150 [==============================] - 0s 379us/step - loss: 0.5130 - acc: 0.7200\n",
      "Epoch 45/100\n",
      "150/150 [==============================] - 0s 262us/step - loss: 0.5060 - acc: 0.7467\n",
      "Epoch 46/100\n",
      "150/150 [==============================] - 0s 259us/step - loss: 0.4994 - acc: 0.7467\n",
      "Epoch 47/100\n",
      "150/150 [==============================] - 0s 254us/step - loss: 0.4930 - acc: 0.7467\n",
      "Epoch 48/100\n",
      "150/150 [==============================] - 0s 233us/step - loss: 0.4870 - acc: 0.7533\n",
      "Epoch 49/100\n",
      "150/150 [==============================] - 0s 233us/step - loss: 0.4813 - acc: 0.7667\n",
      "Epoch 50/100\n",
      "150/150 [==============================] - 0s 286us/step - loss: 0.4756 - acc: 0.7733\n",
      "Epoch 51/100\n",
      "150/150 [==============================] - 0s 243us/step - loss: 0.4703 - acc: 0.7800\n",
      "Epoch 52/100\n",
      "150/150 [==============================] - 0s 505us/step - loss: 0.4652 - acc: 0.7800\n",
      "Epoch 53/100\n",
      "150/150 [==============================] - 0s 259us/step - loss: 0.4598 - acc: 0.7933\n",
      "Epoch 54/100\n",
      "150/150 [==============================] - 0s 233us/step - loss: 0.4551 - acc: 0.8133\n",
      "Epoch 55/100\n",
      "150/150 [==============================] - 0s 246us/step - loss: 0.4503 - acc: 0.8333\n",
      "Epoch 56/100\n",
      "150/150 [==============================] - 0s 233us/step - loss: 0.4458 - acc: 0.8533\n",
      "Epoch 57/100\n",
      "150/150 [==============================] - 0s 296us/step - loss: 0.4411 - acc: 0.8533\n",
      "Epoch 58/100\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.4456 - acc: 0.700 - 0s 299us/step - loss: 0.4374 - acc: 0.8533\n",
      "Epoch 59/100\n",
      "150/150 [==============================] - 0s 348us/step - loss: 0.4337 - acc: 0.8533\n",
      "Epoch 60/100\n",
      "150/150 [==============================] - 0s 253us/step - loss: 0.4288 - acc: 0.8733\n",
      "Epoch 61/100\n",
      "150/150 [==============================] - 0s 206us/step - loss: 0.4244 - acc: 0.8733\n",
      "Epoch 62/100\n",
      "150/150 [==============================] - 0s 312us/step - loss: 0.4205 - acc: 0.8800\n",
      "Epoch 63/100\n",
      "150/150 [==============================] - 0s 239us/step - loss: 0.4167 - acc: 0.8867\n",
      "Epoch 64/100\n",
      "150/150 [==============================] - 0s 239us/step - loss: 0.4130 - acc: 0.8933\n",
      "Epoch 65/100\n",
      "150/150 [==============================] - 0s 246us/step - loss: 0.4093 - acc: 0.9000\n",
      "Epoch 66/100\n",
      "150/150 [==============================] - 0s 253us/step - loss: 0.4062 - acc: 0.9000\n",
      "Epoch 67/100\n",
      "150/150 [==============================] - 0s 266us/step - loss: 0.4024 - acc: 0.9067\n",
      "Epoch 68/100\n",
      "150/150 [==============================] - 0s 273us/step - loss: 0.3994 - acc: 0.9267\n",
      "Epoch 69/100\n",
      "150/150 [==============================] - 0s 246us/step - loss: 0.3960 - acc: 0.9267\n",
      "Epoch 70/100\n",
      "150/150 [==============================] - 0s 360us/step - loss: 0.3929 - acc: 0.9133\n",
      "Epoch 71/100\n",
      "150/150 [==============================] - 0s 253us/step - loss: 0.3892 - acc: 0.9267\n",
      "Epoch 72/100\n",
      "150/150 [==============================] - 0s 326us/step - loss: 0.3862 - acc: 0.9267\n",
      "Epoch 73/100\n",
      "150/150 [==============================] - 0s 319us/step - loss: 0.3831 - acc: 0.9333\n",
      "Epoch 74/100\n",
      "150/150 [==============================] - 0s 381us/step - loss: 0.3804 - acc: 0.9333\n",
      "Epoch 75/100\n",
      "150/150 [==============================] - 0s 242us/step - loss: 0.3772 - acc: 0.9333\n",
      "Epoch 76/100\n",
      "150/150 [==============================] - 0s 444us/step - loss: 0.3739 - acc: 0.9333\n",
      "Epoch 77/100\n",
      "150/150 [==============================] - 0s 272us/step - loss: 0.3711 - acc: 0.9333\n",
      "Epoch 78/100\n",
      "150/150 [==============================] - 0s 253us/step - loss: 0.3684 - acc: 0.9400\n",
      "Epoch 79/100\n",
      "150/150 [==============================] - 0s 286us/step - loss: 0.3670 - acc: 0.9333\n",
      "Epoch 80/100\n",
      "150/150 [==============================] - 0s 246us/step - loss: 0.3626 - acc: 0.9400\n",
      "Epoch 81/100\n",
      "150/150 [==============================] - 0s 412us/step - loss: 0.3598 - acc: 0.9400\n",
      "Epoch 82/100\n",
      "150/150 [==============================] - 0s 332us/step - loss: 0.3571 - acc: 0.9400\n",
      "Epoch 83/100\n",
      "150/150 [==============================] - 0s 363us/step - loss: 0.3546 - acc: 0.9400\n",
      "Epoch 84/100\n",
      "150/150 [==============================] - 0s 517us/step - loss: 0.3522 - acc: 0.9400\n",
      "Epoch 85/100\n",
      "150/150 [==============================] - 0s 253us/step - loss: 0.3494 - acc: 0.9400\n",
      "Epoch 86/100\n",
      "150/150 [==============================] - 0s 279us/step - loss: 0.3473 - acc: 0.9400\n",
      "Epoch 87/100\n",
      "150/150 [==============================] - 0s 266us/step - loss: 0.3439 - acc: 0.9400\n",
      "Epoch 88/100\n",
      "150/150 [==============================] - 0s 260us/step - loss: 0.3415 - acc: 0.9400\n",
      "Epoch 89/100\n",
      "150/150 [==============================] - 0s 299us/step - loss: 0.3390 - acc: 0.9400\n",
      "Epoch 90/100\n",
      "150/150 [==============================] - 0s 246us/step - loss: 0.3366 - acc: 0.9400\n",
      "Epoch 91/100\n",
      "150/150 [==============================] - 0s 239us/step - loss: 0.3341 - acc: 0.9467\n",
      "Epoch 92/100\n",
      "150/150 [==============================] - 0s 605us/step - loss: 0.3318 - acc: 0.9400\n",
      "Epoch 93/100\n",
      "150/150 [==============================] - 0s 266us/step - loss: 0.3301 - acc: 0.9400\n",
      "Epoch 94/100\n",
      "150/150 [==============================] - 0s 299us/step - loss: 0.3270 - acc: 0.9467\n",
      "Epoch 95/100\n",
      "150/150 [==============================] - 0s 315us/step - loss: 0.3247 - acc: 0.9467\n",
      "Epoch 96/100\n",
      "150/150 [==============================] - 0s 346us/step - loss: 0.3226 - acc: 0.9400\n",
      "Epoch 97/100\n",
      "150/150 [==============================] - 0s 312us/step - loss: 0.3202 - acc: 0.9400\n",
      "Epoch 98/100\n",
      "150/150 [==============================] - 0s 226us/step - loss: 0.3178 - acc: 0.9400\n",
      "Epoch 99/100\n",
      "150/150 [==============================] - 0s 279us/step - loss: 0.3162 - acc: 0.9467\n",
      "Epoch 100/100\n",
      "150/150 [==============================] - 0s 286us/step - loss: 0.3133 - acc: 0.9467\n",
      "최적스코어 : 0.9000  사용한 파라미터 조합 : {'classifier__batch_size': 10, 'classifier__epochs': 100}\n",
      "0.52 (0.21)    with {'classifier__batch_size': 10, 'classifier__epochs': 10}\n",
      "0.70 (0.10)    with {'classifier__batch_size': 10, 'classifier__epochs': 50}\n",
      "0.90 (0.12)    with {'classifier__batch_size': 10, 'classifier__epochs': 100}\n",
      "0.57 (0.24)    with {'classifier__batch_size': 20, 'classifier__epochs': 10}\n",
      "0.73 (0.15)    with {'classifier__batch_size': 20, 'classifier__epochs': 50}\n",
      "0.71 (0.16)    with {'classifier__batch_size': 20, 'classifier__epochs': 100}\n",
      "0.51 (0.19)    with {'classifier__batch_size': 30, 'classifier__epochs': 10}\n",
      "0.51 (0.20)    with {'classifier__batch_size': 30, 'classifier__epochs': 50}\n",
      "0.73 (0.13)    with {'classifier__batch_size': 30, 'classifier__epochs': 100}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "estimators = []\n",
    "estimators.append(('MinMax', MinMaxScaler()))\n",
    "estimators.append(('classifier', KerasClassifier(build_fn=baseline_model, verbose=1 )))\n",
    "pipeline = Pipeline(estimators)\n",
    "param_grid = {'classifier__batch_size':batch_size, 'classifier__epochs':epochs}\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X,Y)\n",
    "print(\"최적스코어 : {:.4f}  사용한 파라미터 조합 : {}\".format(grid_result.best_score_,grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, std, param in zip(means, stds, params):\n",
    "    print(\"{:.2f} ({:.2f})    with {}\".format(mean,std,param))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다양한 모델 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(init_mode = 'normal', optimizer = 'relu') :\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=4, kernel_initializer=init_mode, activation=optimizer))\n",
    "    model.add(Dense(3, activation = 'softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def larger_model(init_mode = 'normal', optimizer = 'relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_dim=4, kernel_initializer=init_mode, activation=optimizer))\n",
    "    model.add(Dense(6, kernel_initializer=init_mode, activation=optimizer))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam' , metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def wider_model(init_mode = 'normal', optimizer = 'relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=4, kernel_initializer=init_mode, activation=optimizer))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam' , metrics=['accuracy'])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_mode = ['uniform','normal','glorot_normal'] \n",
    "optimizer = ['relu', 'softplus','tanh','sigmoid']\n",
    "batch_size = [10,20,30]\n",
    "epochs = [10,50,100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- baseline_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\happy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\happy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\happy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\happy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\happy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\happy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_list = [baseline_model(), larger_model(),wider_model()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 1.1729 - acc: 0.3333\n",
      "Epoch 2/100\n",
      "150/150 [==============================] - 0s 219us/step - loss: 1.1357 - acc: 0.3333\n",
      "Epoch 3/100\n",
      "150/150 [==============================] - 0s 279us/step - loss: 1.1037 - acc: 0.3333\n",
      "Epoch 4/100\n",
      "150/150 [==============================] - 0s 293us/step - loss: 1.0801 - acc: 0.3333\n",
      "Epoch 5/100\n",
      "150/150 [==============================] - 0s 246us/step - loss: 1.0576 - acc: 0.3333\n",
      "Epoch 6/100\n",
      "150/150 [==============================] - 0s 339us/step - loss: 1.0399 - acc: 0.3267\n",
      "Epoch 7/100\n",
      "150/150 [==============================] - 0s 299us/step - loss: 1.0228 - acc: 0.2800\n",
      "Epoch 8/100\n",
      "150/150 [==============================] - 0s 279us/step - loss: 1.0076 - acc: 0.2667\n",
      "Epoch 9/100\n",
      "150/150 [==============================] - 0s 286us/step - loss: 0.9911 - acc: 0.2600\n",
      "Epoch 10/100\n",
      "150/150 [==============================] - 0s 379us/step - loss: 0.9764 - acc: 0.3067\n",
      "Epoch 11/100\n",
      "150/150 [==============================] - 0s 505us/step - loss: 0.9615 - acc: 0.3067\n",
      "Epoch 12/100\n",
      "150/150 [==============================] - 0s 505us/step - loss: 0.9470 - acc: 0.3600\n",
      "Epoch 13/100\n",
      "150/150 [==============================] - 0s 253us/step - loss: 0.9318 - acc: 0.4133\n",
      "Epoch 14/100\n",
      "150/150 [==============================] - 0s 406us/step - loss: 0.9171 - acc: 0.4600\n",
      "Epoch 15/100\n",
      "150/150 [==============================] - 0s 369us/step - loss: 0.9016 - acc: 0.5333\n",
      "Epoch 16/100\n",
      "150/150 [==============================] - 0s 413us/step - loss: 0.8866 - acc: 0.5867\n",
      "Epoch 17/100\n",
      "150/150 [==============================] - 0s 491us/step - loss: 0.8713 - acc: 0.6133\n",
      "Epoch 18/100\n",
      "150/150 [==============================] - 0s 525us/step - loss: 0.8554 - acc: 0.6267\n",
      "Epoch 19/100\n",
      "150/150 [==============================] - 0s 452us/step - loss: 0.8404 - acc: 0.6467\n",
      "Epoch 20/100\n",
      "150/150 [==============================] - 0s 293us/step - loss: 0.8238 - acc: 0.6467\n",
      "Epoch 21/100\n",
      "150/150 [==============================] - 0s 266us/step - loss: 0.8085 - acc: 0.6533\n",
      "Epoch 22/100\n",
      "150/150 [==============================] - 0s 318us/step - loss: 0.7936 - acc: 0.6667\n",
      "Epoch 23/100\n",
      "150/150 [==============================] - 0s 273us/step - loss: 0.7779 - acc: 0.6733\n",
      "Epoch 24/100\n",
      "150/150 [==============================] - 0s 279us/step - loss: 0.7629 - acc: 0.6867\n",
      "Epoch 25/100\n",
      "150/150 [==============================] - 0s 346us/step - loss: 0.7484 - acc: 0.6867\n",
      "Epoch 26/100\n",
      "150/150 [==============================] - 0s 279us/step - loss: 0.7338 - acc: 0.6867\n",
      "Epoch 27/100\n",
      "150/150 [==============================] - 0s 286us/step - loss: 0.7194 - acc: 0.7000\n",
      "Epoch 28/100\n",
      "150/150 [==============================] - 0s 372us/step - loss: 0.7055 - acc: 0.7000\n",
      "Epoch 29/100\n",
      "150/150 [==============================] - 0s 309us/step - loss: 0.6920 - acc: 0.7133\n",
      "Epoch 30/100\n",
      "150/150 [==============================] - 0s 239us/step - loss: 0.6791 - acc: 0.7133\n",
      "Epoch 31/100\n",
      "150/150 [==============================] - 0s 293us/step - loss: 0.6663 - acc: 0.7333\n",
      "Epoch 32/100\n",
      "150/150 [==============================] - 0s 266us/step - loss: 0.6545 - acc: 0.7400\n",
      "Epoch 33/100\n",
      "150/150 [==============================] - 0s 299us/step - loss: 0.6421 - acc: 0.7400\n",
      "Epoch 34/100\n",
      "150/150 [==============================] - 0s 279us/step - loss: 0.6306 - acc: 0.7533\n",
      "Epoch 35/100\n",
      "150/150 [==============================] - 0s 293us/step - loss: 0.6200 - acc: 0.7533\n",
      "Epoch 36/100\n",
      "150/150 [==============================] - 0s 259us/step - loss: 0.6092 - acc: 0.7667\n",
      "Epoch 37/100\n",
      "150/150 [==============================] - 0s 279us/step - loss: 0.5988 - acc: 0.7667\n",
      "Epoch 38/100\n",
      "150/150 [==============================] - 0s 272us/step - loss: 0.5894 - acc: 0.7667\n",
      "Epoch 39/100\n",
      "150/150 [==============================] - 0s 299us/step - loss: 0.5798 - acc: 0.7800\n",
      "Epoch 40/100\n",
      "150/150 [==============================] - 0s 272us/step - loss: 0.5713 - acc: 0.7667\n",
      "Epoch 41/100\n",
      "150/150 [==============================] - 0s 280us/step - loss: 0.5626 - acc: 0.7667\n",
      "Epoch 42/100\n",
      "150/150 [==============================] - 0s 337us/step - loss: 0.5539 - acc: 0.7800\n",
      "Epoch 43/100\n",
      "150/150 [==============================] - 0s 313us/step - loss: 0.5469 - acc: 0.7867\n",
      "Epoch 44/100\n",
      "150/150 [==============================] - 0s 275us/step - loss: 0.5384 - acc: 0.7933\n",
      "Epoch 45/100\n",
      "150/150 [==============================] - 0s 360us/step - loss: 0.5310 - acc: 0.7800\n",
      "Epoch 46/100\n",
      "150/150 [==============================] - 0s 279us/step - loss: 0.5244 - acc: 0.7867\n",
      "Epoch 47/100\n",
      "150/150 [==============================] - 0s 318us/step - loss: 0.5171 - acc: 0.7800\n",
      "Epoch 48/100\n",
      "150/150 [==============================] - 0s 372us/step - loss: 0.5106 - acc: 0.8000\n",
      "Epoch 49/100\n",
      "150/150 [==============================] - 0s 326us/step - loss: 0.5043 - acc: 0.8267\n",
      "Epoch 50/100\n",
      "150/150 [==============================] - 0s 246us/step - loss: 0.4980 - acc: 0.8200\n",
      "Epoch 51/100\n",
      "150/150 [==============================] - 0s 326us/step - loss: 0.4919 - acc: 0.8200\n",
      "Epoch 52/100\n",
      "150/150 [==============================] - 0s 239us/step - loss: 0.4864 - acc: 0.8267\n",
      "Epoch 53/100\n",
      "150/150 [==============================] - 0s 271us/step - loss: 0.4811 - acc: 0.8333\n",
      "Epoch 54/100\n",
      "150/150 [==============================] - 0s 266us/step - loss: 0.4757 - acc: 0.8600\n",
      "Epoch 55/100\n",
      "150/150 [==============================] - 0s 265us/step - loss: 0.4698 - acc: 0.8600\n",
      "Epoch 56/100\n",
      "150/150 [==============================] - 0s 273us/step - loss: 0.4648 - acc: 0.8533\n",
      "Epoch 57/100\n",
      "150/150 [==============================] - 0s 312us/step - loss: 0.4596 - acc: 0.8533\n",
      "Epoch 58/100\n",
      "150/150 [==============================] - 0s 273us/step - loss: 0.4546 - acc: 0.8533\n",
      "Epoch 59/100\n",
      "150/150 [==============================] - 0s 246us/step - loss: 0.4500 - acc: 0.8533\n",
      "Epoch 60/100\n",
      "150/150 [==============================] - 0s 474us/step - loss: 0.4450 - acc: 0.8533\n",
      "Epoch 61/100\n",
      "150/150 [==============================] - 0s 299us/step - loss: 0.4405 - acc: 0.8600\n",
      "Epoch 62/100\n",
      "150/150 [==============================] - 0s 332us/step - loss: 0.4369 - acc: 0.8600\n",
      "Epoch 63/100\n",
      "150/150 [==============================] - 0s 271us/step - loss: 0.4316 - acc: 0.8667\n",
      "Epoch 64/100\n",
      "150/150 [==============================] - 0s 259us/step - loss: 0.4273 - acc: 0.8600\n",
      "Epoch 65/100\n",
      "150/150 [==============================] - 0s 246us/step - loss: 0.4229 - acc: 0.8600\n",
      "Epoch 66/100\n",
      "150/150 [==============================] - 0s 299us/step - loss: 0.4192 - acc: 0.8667\n",
      "Epoch 67/100\n",
      "150/150 [==============================] - 0s 299us/step - loss: 0.4156 - acc: 0.8800\n",
      "Epoch 68/100\n",
      "150/150 [==============================] - 0s 299us/step - loss: 0.4112 - acc: 0.8933\n",
      "Epoch 69/100\n",
      "150/150 [==============================] - 0s 276us/step - loss: 0.4067 - acc: 0.8933\n",
      "Epoch 70/100\n",
      "150/150 [==============================] - 0s 326us/step - loss: 0.4029 - acc: 0.9133\n",
      "Epoch 71/100\n",
      "150/150 [==============================] - 0s 226us/step - loss: 0.3990 - acc: 0.9067\n",
      "Epoch 72/100\n",
      "150/150 [==============================] - 0s 253us/step - loss: 0.3953 - acc: 0.9067\n",
      "Epoch 73/100\n",
      "150/150 [==============================] - 0s 246us/step - loss: 0.3915 - acc: 0.9067\n",
      "Epoch 74/100\n",
      "150/150 [==============================] - 0s 329us/step - loss: 0.3889 - acc: 0.9067\n",
      "Epoch 75/100\n",
      "150/150 [==============================] - 0s 337us/step - loss: 0.3836 - acc: 0.9133\n",
      "Epoch 76/100\n",
      "150/150 [==============================] - 0s 238us/step - loss: 0.3806 - acc: 0.9267\n",
      "Epoch 77/100\n",
      "150/150 [==============================] - 0s 253us/step - loss: 0.3763 - acc: 0.9267\n",
      "Epoch 78/100\n",
      "150/150 [==============================] - 0s 312us/step - loss: 0.3730 - acc: 0.9000\n",
      "Epoch 79/100\n",
      "150/150 [==============================] - 0s 239us/step - loss: 0.3696 - acc: 0.9067\n",
      "Epoch 80/100\n",
      "150/150 [==============================] - 0s 265us/step - loss: 0.3661 - acc: 0.9067\n",
      "Epoch 81/100\n",
      "150/150 [==============================] - 0s 299us/step - loss: 0.3626 - acc: 0.9400\n",
      "Epoch 82/100\n",
      "150/150 [==============================] - 0s 249us/step - loss: 0.3591 - acc: 0.9333\n",
      "Epoch 83/100\n",
      "150/150 [==============================] - 0s 261us/step - loss: 0.3557 - acc: 0.9333\n",
      "Epoch 84/100\n",
      "150/150 [==============================] - 0s 339us/step - loss: 0.3525 - acc: 0.9333\n",
      "Epoch 85/100\n",
      "150/150 [==============================] - 0s 238us/step - loss: 0.3491 - acc: 0.9333\n",
      "Epoch 86/100\n",
      "150/150 [==============================] - 0s 273us/step - loss: 0.3461 - acc: 0.9400\n",
      "Epoch 87/100\n",
      "150/150 [==============================] - 0s 253us/step - loss: 0.3424 - acc: 0.9467\n",
      "Epoch 88/100\n",
      "150/150 [==============================] - 0s 246us/step - loss: 0.3396 - acc: 0.9333\n",
      "Epoch 89/100\n",
      "150/150 [==============================] - 0s 346us/step - loss: 0.3362 - acc: 0.9533\n",
      "Epoch 90/100\n",
      "150/150 [==============================] - 0s 273us/step - loss: 0.3330 - acc: 0.9467\n",
      "Epoch 91/100\n",
      "150/150 [==============================] - 0s 239us/step - loss: 0.3295 - acc: 0.9467\n",
      "Epoch 92/100\n",
      "150/150 [==============================] - 0s 233us/step - loss: 0.3265 - acc: 0.9400\n",
      "Epoch 93/100\n",
      "150/150 [==============================] - 0s 266us/step - loss: 0.3239 - acc: 0.9400\n",
      "Epoch 94/100\n",
      "150/150 [==============================] - 0s 253us/step - loss: 0.3202 - acc: 0.9533\n",
      "Epoch 95/100\n",
      "150/150 [==============================] - 0s 372us/step - loss: 0.3177 - acc: 0.9533\n",
      "Epoch 96/100\n",
      "150/150 [==============================] - 0s 328us/step - loss: 0.3142 - acc: 0.9467\n",
      "Epoch 97/100\n",
      "150/150 [==============================] - 0s 233us/step - loss: 0.3111 - acc: 0.9467\n",
      "Epoch 98/100\n",
      "150/150 [==============================] - 0s 233us/step - loss: 0.3082 - acc: 0.9467\n",
      "Epoch 99/100\n",
      "150/150 [==============================] - 0s 253us/step - loss: 0.3051 - acc: 0.9533\n",
      "Epoch 100/100\n",
      "150/150 [==============================] - 0s 246us/step - loss: 0.3024 - acc: 0.9533\n",
      "최적스코어 : 0.9533  사용한 파라미터 조합 : {'classifier__init_mode': 'glorot_normal', 'classifier__optimizer': 'tanh'}\n",
      "0.82 (0.16)    with {'classifier__init_mode': 'uniform', 'classifier__optimizer': 'relu'}\n",
      "0.88 (0.07)    with {'classifier__init_mode': 'uniform', 'classifier__optimizer': 'softplus'}\n",
      "0.94 (0.04)    with {'classifier__init_mode': 'uniform', 'classifier__optimizer': 'softsign'}\n",
      "0.93 (0.04)    with {'classifier__init_mode': 'uniform', 'classifier__optimizer': 'tanh'}\n",
      "0.71 (0.12)    with {'classifier__init_mode': 'uniform', 'classifier__optimizer': 'sigmoid'}\n",
      "0.94 (0.04)    with {'classifier__init_mode': 'lecun_uniform', 'classifier__optimizer': 'relu'}\n",
      "0.87 (0.10)    with {'classifier__init_mode': 'lecun_uniform', 'classifier__optimizer': 'softplus'}\n",
      "0.91 (0.08)    with {'classifier__init_mode': 'lecun_uniform', 'classifier__optimizer': 'softsign'}\n",
      "0.94 (0.02)    with {'classifier__init_mode': 'lecun_uniform', 'classifier__optimizer': 'tanh'}\n",
      "0.75 (0.15)    with {'classifier__init_mode': 'lecun_uniform', 'classifier__optimizer': 'sigmoid'}\n",
      "0.92 (0.04)    with {'classifier__init_mode': 'normal', 'classifier__optimizer': 'relu'}\n",
      "0.89 (0.08)    with {'classifier__init_mode': 'normal', 'classifier__optimizer': 'softplus'}\n",
      "0.95 (0.05)    with {'classifier__init_mode': 'normal', 'classifier__optimizer': 'softsign'}\n",
      "0.95 (0.05)    with {'classifier__init_mode': 'normal', 'classifier__optimizer': 'tanh'}\n",
      "0.75 (0.10)    with {'classifier__init_mode': 'normal', 'classifier__optimizer': 'sigmoid'}\n",
      "0.87 (0.08)    with {'classifier__init_mode': 'glorot_normal', 'classifier__optimizer': 'relu'}\n",
      "0.89 (0.05)    with {'classifier__init_mode': 'glorot_normal', 'classifier__optimizer': 'softplus'}\n",
      "0.93 (0.04)    with {'classifier__init_mode': 'glorot_normal', 'classifier__optimizer': 'softsign'}\n",
      "0.95 (0.05)    with {'classifier__init_mode': 'glorot_normal', 'classifier__optimizer': 'tanh'}\n",
      "0.76 (0.14)    with {'classifier__init_mode': 'glorot_normal', 'classifier__optimizer': 'sigmoid'}\n"
     ]
    }
   ],
   "source": [
    "estimators = []\n",
    "estimators.append(('MinMax', MinMaxScaler()))\n",
    "estimators.append(('classifier', KerasClassifier(build_fn=baseline_model,epochs=100,batch_size=10, verbose=1 )))\n",
    "pipeline = Pipeline(estimators)\n",
    "param_grid = {#'classifier__batch_size':batch_size,'classifier__epochs':epochs,\n",
    "         \"classifier__init_mode\":init_mode,'classifier__optimizer':optimizer}\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X,Y) #'classifier_validation_split'=0.3)\n",
    "print(\"최적스코어 : {:.4f}  사용한 파라미터 조합 : {}\".format(grid_result.best_score_,grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, std, param in zip(means, stds, params):\n",
    "    print(\"{:.2f} ({:.2f})    with {}\".format(mean,std,param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- larger_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\happy\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/100\n",
      "150/150 [==============================] - 1s 6ms/step - loss: 1.0956 - acc: 0.3333\n",
      "Epoch 2/100\n",
      "150/150 [==============================] - 0s 180us/step - loss: 1.0852 - acc: 0.3333\n",
      "Epoch 3/100\n",
      "150/150 [==============================] - 0s 286us/step - loss: 1.0664 - acc: 0.3733\n",
      "Epoch 4/100\n",
      "150/150 [==============================] - 0s 206us/step - loss: 1.0348 - acc: 0.5733\n",
      "Epoch 5/100\n",
      "150/150 [==============================] - 0s 253us/step - loss: 0.9870 - acc: 0.6333\n",
      "Epoch 6/100\n",
      "150/150 [==============================] - 0s 200us/step - loss: 0.9231 - acc: 0.6667\n",
      "Epoch 7/100\n",
      "150/150 [==============================] - 0s 246us/step - loss: 0.8427 - acc: 0.6667\n",
      "Epoch 8/100\n",
      "150/150 [==============================] - 0s 279us/step - loss: 0.7577 - acc: 0.6667\n",
      "Epoch 9/100\n",
      "150/150 [==============================] - 0s 226us/step - loss: 0.6791 - acc: 0.6667\n",
      "Epoch 10/100\n",
      "150/150 [==============================] - 0s 233us/step - loss: 0.6130 - acc: 0.6667\n",
      "Epoch 11/100\n",
      "150/150 [==============================] - 0s 213us/step - loss: 0.5649 - acc: 0.6733\n",
      "Epoch 12/100\n",
      "150/150 [==============================] - 0s 239us/step - loss: 0.5310 - acc: 0.6800\n",
      "Epoch 13/100\n",
      "150/150 [==============================] - 0s 213us/step - loss: 0.5055 - acc: 0.6867\n",
      "Epoch 14/100\n",
      "150/150 [==============================] - 0s 233us/step - loss: 0.4867 - acc: 0.6867\n",
      "Epoch 15/100\n",
      "150/150 [==============================] - 0s 319us/step - loss: 0.4724 - acc: 0.6933\n",
      "Epoch 16/100\n",
      "150/150 [==============================] - 0s 299us/step - loss: 0.4606 - acc: 0.6933\n",
      "Epoch 17/100\n",
      "150/150 [==============================] - 0s 193us/step - loss: 0.4511 - acc: 0.7000\n",
      "Epoch 18/100\n",
      "150/150 [==============================] - 0s 266us/step - loss: 0.4416 - acc: 0.7133\n",
      "Epoch 19/100\n",
      "150/150 [==============================] - 0s 253us/step - loss: 0.4337 - acc: 0.7333\n",
      "Epoch 20/100\n",
      "150/150 [==============================] - 0s 219us/step - loss: 0.4262 - acc: 0.7467\n",
      "Epoch 21/100\n",
      "150/150 [==============================] - 0s 293us/step - loss: 0.4197 - acc: 0.7733\n",
      "Epoch 22/100\n",
      "150/150 [==============================] - 0s 239us/step - loss: 0.4130 - acc: 0.7733\n",
      "Epoch 23/100\n",
      "150/150 [==============================] - 0s 193us/step - loss: 0.4064 - acc: 0.7800\n",
      "Epoch 24/100\n",
      "150/150 [==============================] - 0s 266us/step - loss: 0.4000 - acc: 0.8133\n",
      "Epoch 25/100\n",
      "150/150 [==============================] - 0s 186us/step - loss: 0.3939 - acc: 0.8533\n",
      "Epoch 26/100\n",
      "150/150 [==============================] - 0s 239us/step - loss: 0.3879 - acc: 0.8533\n",
      "Epoch 27/100\n",
      "150/150 [==============================] - 0s 253us/step - loss: 0.3814 - acc: 0.8667\n",
      "Epoch 28/100\n",
      "150/150 [==============================] - 0s 199us/step - loss: 0.3750 - acc: 0.8800\n",
      "Epoch 29/100\n",
      "150/150 [==============================] - 0s 226us/step - loss: 0.3694 - acc: 0.8800\n",
      "Epoch 30/100\n",
      "150/150 [==============================] - 0s 233us/step - loss: 0.3623 - acc: 0.8933\n",
      "Epoch 31/100\n",
      "150/150 [==============================] - 0s 226us/step - loss: 0.3559 - acc: 0.9067\n",
      "Epoch 32/100\n",
      "150/150 [==============================] - 0s 213us/step - loss: 0.3493 - acc: 0.9267\n",
      "Epoch 33/100\n",
      "150/150 [==============================] - 0s 246us/step - loss: 0.3423 - acc: 0.9333\n",
      "Epoch 34/100\n",
      "150/150 [==============================] - 0s 293us/step - loss: 0.3361 - acc: 0.9333\n",
      "Epoch 35/100\n",
      "150/150 [==============================] - 0s 219us/step - loss: 0.3283 - acc: 0.9333\n",
      "Epoch 36/100\n",
      "150/150 [==============================] - 0s 220us/step - loss: 0.3209 - acc: 0.9333\n",
      "Epoch 37/100\n",
      "150/150 [==============================] - 0s 253us/step - loss: 0.3135 - acc: 0.9400\n",
      "Epoch 38/100\n",
      "150/150 [==============================] - 0s 299us/step - loss: 0.3060 - acc: 0.9467\n",
      "Epoch 39/100\n",
      "150/150 [==============================] - 0s 226us/step - loss: 0.2987 - acc: 0.9400\n",
      "Epoch 40/100\n",
      "150/150 [==============================] - 0s 266us/step - loss: 0.2916 - acc: 0.9533\n",
      "Epoch 41/100\n",
      "150/150 [==============================] - 0s 226us/step - loss: 0.2830 - acc: 0.9533\n",
      "Epoch 42/100\n",
      "150/150 [==============================] - 0s 226us/step - loss: 0.2769 - acc: 0.9400\n",
      "Epoch 43/100\n",
      "150/150 [==============================] - 0s 219us/step - loss: 0.2671 - acc: 0.9733\n",
      "Epoch 44/100\n",
      "150/150 [==============================] - 0s 199us/step - loss: 0.2603 - acc: 0.9733\n",
      "Epoch 45/100\n",
      "150/150 [==============================] - 0s 273us/step - loss: 0.2523 - acc: 0.9733\n",
      "Epoch 46/100\n",
      "150/150 [==============================] - 0s 219us/step - loss: 0.2486 - acc: 0.9733\n",
      "Epoch 47/100\n",
      "150/150 [==============================] - 0s 233us/step - loss: 0.2376 - acc: 0.9733\n",
      "Epoch 48/100\n",
      "150/150 [==============================] - 0s 219us/step - loss: 0.2301 - acc: 0.9733\n",
      "Epoch 49/100\n",
      "150/150 [==============================] - 0s 193us/step - loss: 0.2236 - acc: 0.9733\n",
      "Epoch 50/100\n",
      "150/150 [==============================] - 0s 306us/step - loss: 0.2183 - acc: 0.9733\n",
      "Epoch 51/100\n",
      "150/150 [==============================] - 0s 286us/step - loss: 0.2117 - acc: 0.9733\n",
      "Epoch 52/100\n",
      "150/150 [==============================] - 0s 193us/step - loss: 0.2080 - acc: 0.9600\n",
      "Epoch 53/100\n",
      "150/150 [==============================] - 0s 213us/step - loss: 0.1997 - acc: 0.9733\n",
      "Epoch 54/100\n",
      "150/150 [==============================] - 0s 206us/step - loss: 0.1919 - acc: 0.9733\n",
      "Epoch 55/100\n",
      "150/150 [==============================] - 0s 193us/step - loss: 0.1875 - acc: 0.9667\n",
      "Epoch 56/100\n",
      "150/150 [==============================] - 0s 346us/step - loss: 0.1839 - acc: 0.9733\n",
      "Epoch 57/100\n",
      "150/150 [==============================] - 0s 226us/step - loss: 0.1772 - acc: 0.9733\n",
      "Epoch 58/100\n",
      "150/150 [==============================] - 0s 233us/step - loss: 0.1727 - acc: 0.9667\n",
      "Epoch 59/100\n",
      "150/150 [==============================] - 0s 246us/step - loss: 0.1682 - acc: 0.9733\n",
      "Epoch 60/100\n",
      "150/150 [==============================] - 0s 213us/step - loss: 0.1662 - acc: 0.9600\n",
      "Epoch 61/100\n",
      "150/150 [==============================] - 0s 239us/step - loss: 0.1585 - acc: 0.9667\n",
      "Epoch 62/100\n",
      "150/150 [==============================] - 0s 206us/step - loss: 0.1555 - acc: 0.9733\n",
      "Epoch 63/100\n",
      "150/150 [==============================] - 0s 239us/step - loss: 0.1520 - acc: 0.9667\n",
      "Epoch 64/100\n",
      "150/150 [==============================] - 0s 306us/step - loss: 0.1483 - acc: 0.9667\n",
      "Epoch 65/100\n",
      "150/150 [==============================] - 0s 239us/step - loss: 0.1447 - acc: 0.9667\n",
      "Epoch 66/100\n",
      "150/150 [==============================] - 0s 180us/step - loss: 0.1440 - acc: 0.9733\n",
      "Epoch 67/100\n",
      "150/150 [==============================] - 0s 219us/step - loss: 0.1390 - acc: 0.9667\n",
      "Epoch 68/100\n",
      "150/150 [==============================] - 0s 219us/step - loss: 0.1360 - acc: 0.9667\n",
      "Epoch 69/100\n",
      "150/150 [==============================] - 0s 193us/step - loss: 0.1323 - acc: 0.9667\n",
      "Epoch 70/100\n",
      "150/150 [==============================] - 0s 226us/step - loss: 0.1289 - acc: 0.9667\n",
      "Epoch 71/100\n",
      "150/150 [==============================] - 0s 180us/step - loss: 0.1265 - acc: 0.9667\n",
      "Epoch 72/100\n",
      "150/150 [==============================] - 0s 200us/step - loss: 0.1253 - acc: 0.9667\n",
      "Epoch 73/100\n",
      "150/150 [==============================] - 0s 246us/step - loss: 0.1220 - acc: 0.9667\n",
      "Epoch 74/100\n",
      "150/150 [==============================] - 0s 246us/step - loss: 0.1207 - acc: 0.9667\n",
      "Epoch 75/100\n",
      "150/150 [==============================] - 0s 246us/step - loss: 0.1214 - acc: 0.9667\n",
      "Epoch 76/100\n",
      "150/150 [==============================] - 0s 199us/step - loss: 0.1166 - acc: 0.9667\n",
      "Epoch 77/100\n",
      "150/150 [==============================] - 0s 193us/step - loss: 0.1142 - acc: 0.9667\n",
      "Epoch 78/100\n",
      "150/150 [==============================] - 0s 259us/step - loss: 0.1121 - acc: 0.9667\n",
      "Epoch 79/100\n",
      "150/150 [==============================] - 0s 213us/step - loss: 0.1098 - acc: 0.9667\n",
      "Epoch 80/100\n",
      "150/150 [==============================] - 0s 445us/step - loss: 0.1081 - acc: 0.9667\n",
      "Epoch 81/100\n",
      "150/150 [==============================] - 0s 186us/step - loss: 0.1063 - acc: 0.9667\n",
      "Epoch 82/100\n",
      "150/150 [==============================] - 0s 199us/step - loss: 0.1047 - acc: 0.9667\n",
      "Epoch 83/100\n",
      "150/150 [==============================] - 0s 193us/step - loss: 0.1030 - acc: 0.9667\n",
      "Epoch 84/100\n",
      "150/150 [==============================] - 0s 200us/step - loss: 0.1014 - acc: 0.9667\n",
      "Epoch 85/100\n",
      "150/150 [==============================] - 0s 213us/step - loss: 0.1000 - acc: 0.9667\n",
      "Epoch 86/100\n",
      "150/150 [==============================] - 0s 186us/step - loss: 0.1009 - acc: 0.9667\n",
      "Epoch 87/100\n",
      "150/150 [==============================] - 0s 273us/step - loss: 0.0985 - acc: 0.9667\n",
      "Epoch 88/100\n",
      "150/150 [==============================] - 0s 253us/step - loss: 0.0971 - acc: 0.9733\n",
      "Epoch 89/100\n",
      "150/150 [==============================] - 0s 199us/step - loss: 0.0996 - acc: 0.9733\n",
      "Epoch 90/100\n",
      "150/150 [==============================] - 0s 193us/step - loss: 0.0945 - acc: 0.9667\n",
      "Epoch 91/100\n",
      "150/150 [==============================] - 0s 239us/step - loss: 0.0930 - acc: 0.9667\n",
      "Epoch 92/100\n",
      "150/150 [==============================] - 0s 206us/step - loss: 0.0921 - acc: 0.9667\n",
      "Epoch 93/100\n",
      "150/150 [==============================] - 0s 186us/step - loss: 0.0901 - acc: 0.9667\n",
      "Epoch 94/100\n",
      "150/150 [==============================] - 0s 253us/step - loss: 0.0902 - acc: 0.9667\n",
      "Epoch 95/100\n",
      "150/150 [==============================] - 0s 352us/step - loss: 0.0905 - acc: 0.9667\n",
      "Epoch 96/100\n",
      "150/150 [==============================] - 0s 193us/step - loss: 0.0910 - acc: 0.9600\n",
      "Epoch 97/100\n",
      "150/150 [==============================] - 0s 199us/step - loss: 0.0882 - acc: 0.9667\n",
      "Epoch 98/100\n",
      "150/150 [==============================] - 0s 213us/step - loss: 0.0855 - acc: 0.9667\n",
      "Epoch 99/100\n",
      "150/150 [==============================] - 0s 233us/step - loss: 0.0847 - acc: 0.9667\n",
      "Epoch 100/100\n",
      "150/150 [==============================] - 0s 199us/step - loss: 0.0844 - acc: 0.9667\n",
      "최적스코어 : 0.9600  사용한 파라미터 조합 : {'classifier__init_mode': 'uniform', 'classifier__optimizer': 'tanh'}\n",
      "0.68 (0.25)    with {'classifier__init_mode': 'uniform', 'classifier__optimizer': 'relu'}\n",
      "0.95 (0.03)    with {'classifier__init_mode': 'uniform', 'classifier__optimizer': 'softplus'}\n",
      "0.96 (0.02)    with {'classifier__init_mode': 'uniform', 'classifier__optimizer': 'tanh'}\n",
      "0.73 (0.16)    with {'classifier__init_mode': 'uniform', 'classifier__optimizer': 'sigmoid'}\n",
      "0.94 (0.06)    with {'classifier__init_mode': 'normal', 'classifier__optimizer': 'relu'}\n",
      "0.94 (0.04)    with {'classifier__init_mode': 'normal', 'classifier__optimizer': 'softplus'}\n",
      "0.95 (0.03)    with {'classifier__init_mode': 'normal', 'classifier__optimizer': 'tanh'}\n",
      "0.49 (0.16)    with {'classifier__init_mode': 'normal', 'classifier__optimizer': 'sigmoid'}\n",
      "0.94 (0.03)    with {'classifier__init_mode': 'glorot_normal', 'classifier__optimizer': 'relu'}\n",
      "0.94 (0.02)    with {'classifier__init_mode': 'glorot_normal', 'classifier__optimizer': 'softplus'}\n",
      "0.95 (0.02)    with {'classifier__init_mode': 'glorot_normal', 'classifier__optimizer': 'tanh'}\n",
      "0.80 (0.17)    with {'classifier__init_mode': 'glorot_normal', 'classifier__optimizer': 'sigmoid'}\n"
     ]
    }
   ],
   "source": [
    "estimators = []\n",
    "estimators.append(('MinMax', MinMaxScaler()))\n",
    "estimators.append(('classifier', KerasClassifier(build_fn=larger_model,epochs=100,batch_size=10, verbose=1 )))\n",
    "pipeline = Pipeline(estimators)\n",
    "param_grid = {#'classifier__batch_size':batch_size,'classifier__epochs':epochs,\n",
    "         \"classifier__init_mode\":init_mode,'classifier__optimizer':optimizer}\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X,Y)\n",
    "print(\"최적스코어 : {:.4f}  사용한 파라미터 조합 : {}\".format(grid_result.best_score_,grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, std, param in zip(means, stds, params):\n",
    "    print(\"{:.2f} ({:.2f})    with {}\".format(mean,std,param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- wider_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = []\n",
    "estimators.append(('MinMax', MinMaxScaler()))\n",
    "estimators.append(('classifier', KerasClassifier(build_fn=wider_model,epochs=100,batch_size=10, verbose=1 )))\n",
    "pipeline = Pipeline(estimators)\n",
    "param_grid = {#'classifier__batch_size':batch_size,'classifier__epochs':epochs,\n",
    "         \"classifier__init_mode\":init_mode,'classifier__optimizer':optimizer}\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X,Y)\n",
    "print(\"최적스코어 : {:.4f}  사용한 파라미터 조합 : {}\".format(grid_result.best_score_,grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, std, param in zip(means, stds, params):\n",
    "    print(\"{:.2f} ({:.2f})    with {}\".format(mean,std,param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
