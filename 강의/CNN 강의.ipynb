{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "\n",
      " 55000 10000 5000\n",
      "\n",
      "train image shape =  (55000, 784)\n",
      "train label shape =  (55000, 10)\n",
      "test image shape =  (10000, 784)\n",
      "test label shape =  (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "from datetime import datetime # datetime.now()를 이용하여 학습 경과 시간 측정\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)\n",
    "\n",
    "print(\"\\n\", mnist.train.num_examples, mnist.test.num_examples, mnist.validation.num_examples)\n",
    "\n",
    "print(\"\\ntrain image shape = \", np.shape(mnist.train.images))\n",
    "print(\"train label shape = \", np.shape(mnist.train.labels))\n",
    "print(\"test image shape = \", np.shape(mnist.test.images))\n",
    "print(\"test label shape = \", np.shape(mnist.test.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-Parameter\n",
    "learning_rate = 0.001\n",
    "epochs = 30\n",
    "batch_size = 100 # 한번에 입력으로 주어지는 MNIST 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력과 정답을 위한 플레이스 홀더 정의\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "T = tf.placeholder(tf.float32,[None, 10])\n",
    "\n",
    "# 입력층의 출력값, 컨볼루션 연산을 위해 reshape 시킴\n",
    "A1 = X_img = tf.reshape(X, [-1,28,28,1]) # image 28x28x1(black/white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1번째 컨볼루션 층\n",
    "# 3x3 크기를 가지는 32개의 필터를 적용\n",
    "\n",
    "F2 = tf.Variable(tf.random_normal([3,3,1,32], stddev=0.01)) # 입력채널(데이터가 들어오는 통로가 1개, 32개 필터)\n",
    "                                                            # 즉 A1은 1개의 통로를 통해서 들어옴\n",
    "                                                            # 3x3의 총 32개 필터가 표준편차 0.01을 갖도록 임의의 값으로 초기화\n",
    "b2 = tf.Variable(tf.constant(0.1, shape = [32])) # 필터가 32개이므로 # constant 정의\n",
    "\n",
    "# 1번째 컨볼루션 연산을 통해 28x28x1 => 28x28x32(필터수)\n",
    "C2 = tf.nn.conv2d(A1, F2, strides=[1,1,1,1], padding = 'SAME')\n",
    "\n",
    "# relu\n",
    "Z2 = tf.nn.relu(C2+b2)\n",
    "\n",
    "# 1번째 max pooling을 통해 28x28x32 => 14x14x32\n",
    "A2 = P2 = tf.nn.max_pool(Z2, ksize = [1,2,2,1], strides=[1,2,2,1], padding='SAME') # 데이터가 부족하면 padding해라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2번째 컨볼루션 층\n",
    "# 3x3 크기를 가지는 64개의 필터를 적용\n",
    "F3 = tf.Variable(tf.random_normal([3,3,32,64], stddev=0.01))\n",
    "b3 = tf.Variable(tf.constant(0.1, shape = [64]))\n",
    "\n",
    "# 2번째 컨볼루션 연산을 통해 14x14x32 => 14x14x64\n",
    "C3 = tf.nn.conv2d(A2, F3, strides = [1,1,1,1], padding = 'SAME')\n",
    "\n",
    "Z3 = tf.nn.relu(C3+b3)\n",
    "\n",
    "# 2번째 max pooling을 통해 14x14x64 => 7x7x64\n",
    "A3 = P3 = tf.nn.max_pool(Z3, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3번째 컨볼루션 층\n",
    "# 3x3크기를 가지는 128개의 필터를 적용\n",
    "F4 = tf.Variable(tf.random_normal([3,3,64,128], stddev=0.01))\n",
    "b4 = tf.Variable(tf.constant(0.1, shape = [128]))\n",
    "\n",
    "# 3번째 컨볼루션 연산을 통해 7x7x64 => 7x7x128\n",
    "C4 = tf.nn.conv2d(A3, F4, strides = [1,1,1,1], padding = 'SAME')\n",
    "\n",
    "Z4 = tf.nn.relu(C4+b4)\n",
    "\n",
    "# 3번째 max pooling을 통해 7x7x128 => 4x4x128\n",
    "A4 = P4 = tf.nn.max_pool(Z4, ksize = [1,2,2,1], strides = [1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 완전연결층\n",
    "# 4x4 크기를 가진 128개의 activation map을 flatten시킴\n",
    "A4_flat = P4_flat = tf.reshape(A4, [-1, 128*4*4]) # 1차원 벡터로 만듬 128*4*4 = 2048개 노드를 가지도록 reshpae\n",
    "# 완전연결층(2048개 노드)과 출력층(10개 노드-one-hot)은 2048x10개 노드가 완전 연결되어 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출력층\n",
    "# 선형회귀를 위한 정의\n",
    "w5 = tf.Variable(tf.random_normal([128*4*4, 10], stddev=0.01))\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "# 출력층 선형회귀 값 z5, 즉 sortmax에 들어가는 입력값\n",
    "z5 = logits = tf.matmul(A4_flat, w5) + b5\n",
    "y = A5 = tf.nn.softmax(z5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=z5, labels=T))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate) # 성능개선을 위해 AdamOptimizer 사용\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size X 10 데이터에 대해 argmax를 통해 행단위로 비교함\n",
    "predicted_val = tf.equal(tf.argmax(A5,1), tf.argmax(T,1))\n",
    "# one-hot인코딩에 의해 출력층 계산값 A5와 정답 T는 batchsize x 10 shape를 가지는 행렬\n",
    "# argmax의 두번째 인자에 1을 주어 행 단위로 A5와 T를 비교\n",
    "\n",
    "# batch_size x 10의 True, False를 1 또는 0으로 반환\n",
    "accuracy = tf.reduce_mean(tf.cast(predicted_val, dtype = tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    start_time = datetime.now()\n",
    "    for i in range(epochs) :\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        for step in range(total_batch) :\n",
    "            batch_x_data, batch_t_data = mnist.train.next_batch(batch_size)\n",
    "            loss_val, _ = sess.run([loss, train], feed_dict = {X: batch_x_data, T : batch_t_data})\n",
    "            if step % 100 == 0:\n",
    "                print(\"epochs = \", i, \", step = \", step, \", loss_val = \", loss_val)\n",
    "                \n",
    "    end_time = datetime.now()\n",
    "    \n",
    "    print(\"\\nelapsed time = \", end_time - start_time)\n",
    "    \n",
    "    # Accuracy 확인\n",
    "    test_x_data = mnist.test.images # 10000x784\n",
    "    test_t_data = mnist.test.labels # 10000x10\n",
    "    \n",
    "    accuary_val = sess.run(accuracy, feed_dict={X: test_x_data, T : test_t_data})\n",
    "    print(\"\\nAccuracy = \", accuracy_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
